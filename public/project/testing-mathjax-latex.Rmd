---
title: Forecasting Capital BikeShare Ridership
author: Robert Hazell
date: '2020-03-31'
slug: forecasting-capital-bikeshare-ridership
categories: []
tags: ["time series", "R", "projects", "machine learning"]
description: ''
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, comment=NA)
# change path to load data and saved models
knitr::opts_knit$set(root.dir = normalizePath("~/Desktop/SMU/bike-sharing-dataset/CapitalBikeShare")) 
```

<center>
![](/project/testing-mathjax-latex_files/capitalbike.png)
</center>

<center>
<p class="caption">
[Capital BikeShare docking station](https://dc.curbed.com/2019/8/22/20828525/capital-bikeshare-dc-college-students-transportation-biking)
</p>

</center>

## Introduction

I love to bike, especially during the spring and summer.  As a New York City resident the city affords miles of urban bike paths, my favorite being the seamless connection between Riverside and Hudson River parks.  Of course thousands (if not millions) of others enjoy biking throughout the country, and if you don't own a bike you can rent one through various bike programs, like NYC's [Citi Bike](https://www.citibikenyc.com/) and Washington DC's [Capital BikeShare](https://www.capitalbikeshare.com/) (CB henceforth).  The latter is the focus of this report.

## Background and Motivation

CB has maintained [ridership data](https://s3.amazonaws.com/capitalbikeshare-data/index.html) since 2010, but I first came across their data on the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) which featured data from 2011-2012.  A more complete analysis of CB's ridership requires more data, so UCI's data will be merged with 2013-2017 data from CB's website.

The objective is to create 30-day trip forecasts by comparing several competing time series models. This task is relevant to CB officials since they could predict how much revenue to expect for any 30 day period.  While location-based data isn't utilized in this analysis, using it could help identify areas needing increased bike availability.  And although daily data is used here, hourly forecasts would enable cyclists to gauge and anticipate future bike demand.

## Literature Review

Bike share programs are relatively new, but research on factors affecting ridership has been conducted.  [Buck and Buehler 2011](https://nacto.org/wp-content/uploads/2012/02/Bike-Lanes-and-Other-Determinants-of-Capital-Bikeshare-Trips-Buck-et-al-12-3539.pdf) have conducted research on CB ridership at the station level during CB's first six months of operation.  Using stepwise multiple regression model, they conclude average daily ridership is affected by the number of bike lines within 1/2 mile of a station, the number of Washington DC Alcohol Beverage Regulation Administration (ABRA) license holders, and the weighted average percentage of households with no access to an automobile (adjusted $R^2$ = 0.66, n=91).  

[Rixey 2012](https://nacto.org/wp-content/uploads/2015/07/2012_Rixey_Station-Level-Forecasting-of-Bike-Sharing-Ridership.pdf) investigates the natural logarithm of monthly bike rentals across three bike share programs (including CB) from the first operating season of each, on a station basis using multiple regression.  Rixey reduces his 19 candidate variables to 11 (plus an intercept) that include various socioeconomic and bike system architecture variables, yielding an adjusted $R^2$ of ~0.8. 

[El-Assi, et al 2017](https://www.researchgate.net/profile/Khandker_Nurul_Habib2/publication/286379583_Effects_of_Built_Environment_and_Weather_on_Bike_Sharing_Demand_A_Station_Level_Analysis_of_Commercial_Bike_Sharing_in_Toronto/links/56684b7c08ae7dc22ad1c992/Effects-of-Built-Environment-and-Weather-on-Bike-Sharing-Demand-A-Station-Level-Analysis-of-Commercial-Bike-Sharing-in-Toronto.pdf) perform a similar types of analysis of Toronto's bike share program, developing models for weekday, weekend, station origin, and station destination ridership.  Adjusted ${R^2}$ values hover around 0.66.

[Wang et al 2012](https://pdfs.semanticscholar.org/8a39/c1df91725ac27ea0c413360c3592b911d17a.pdf) use 19 total socioeconomic variables to develop a regression model for station level, 2011 ridership of the Nice Ride bike share program in Minneapolis.  Notably no weather variables are included.  They achieve an adjusted Adjusted ${R^2}$ of approximately 0.87.  

All of that research focuses more on identifying important variables and trends for ridership but not forecasting (extrapolating) future ridership.  That is the focus of this analysis.  It's important to note high $R^2$ values don't necessarily translate into accurate forecasts since models can overfit the data.  That issue was not examined in their analyses, so it's unsure how predictive their models are.  However, the variables they used may be helpful additions to this analysis (at some later date).

## Data Cleaning and Feature Engineering

Data comes from three sources: UCI 2011-2012 data, historical weather data from NOAA for 2011-2017, and the CB ridership data from 2013-2017.  We'll go through each one; as the code is sufficiently commented, summaries of the data wrangling should suffice.

### UCI Data (2011-2012)

We begin with the UCI data since it's the quickest and serves as the baseline for how the remainder of data from 2013-2017 is handled.  

Relevant revisions include:

* removing unnecessary weather and calendar variables - they'll be replaced later on with other variables
* converting relevant calendar variables into factor/categorical variables
* converting weather data to their original, non-normalized scales
* adding a week of the month column.  This isn't completely necessary now, but it's used later to define/encode holiday dates in the 2013-2017 data

```{r}
# main libraries used; others will be loaded later
library(tswge)
library(fpp2)
library(lubridate)
library(dplyr)

############## Begin with UCI data ##############
# import 2011 and 2012 UCI data - will be merged with 2013-2017 data later
# change file path to match yours!
bikes_uci <- read.csv("day.csv")
# remove atemp since it adds no info on top of temp
# remove season and yr - unnecessary calendar variables
# remove instant; remove workingday - weekday covers this
# remove weathersit - cumbersome to define; this is replaced with actual rain and snowfall data
bikes_uci <- bikes_uci[, -c(1,3,4,8,9,11)]
# rename dteday to TripDate
colnames(bikes_uci)[1] <- "TripDate"
# add 1 to weekday to make consistent with everyday understanding (1-7)
bikes_uci$weekday <- bikes_uci$weekday+1
# convert calendar variables to factor variables
bikes_uci[, 2:4] <- lapply(bikes_uci[, 2:4], factor)
# convert date column to ymd format
bikes_uci$TripDate <- ymd(bikes_uci$TripDate)
# add weekofmon (needed later for the other years' holiday calculations)
bikes_uci <- bikes_uci %>%
  mutate(weekofmon = ifelse(between(day(TripDate),1,7), 1,
                            ifelse(between(day(TripDate),8,14), 2,
                                   ifelse(between(day(TripDate),15,21), 3, 4))))
# un-normalize temp; normalization formula: (t-t_min)/(t_max-t_min); t_max = 39, t_min=-8
bikes_uci$temp <- 47*bikes_uci$temp - 8
# convert temp from celcius to farenheit
bikes_uci$temp <- bikes_uci$temp * 9/5 + 32
# un-normalize humidity; normalization formula: hum/100
bikes_uci$hum <- 100*bikes_uci$hum
# convert weekofmon to factor
bikes_uci$weekofmon <- as.factor(bikes_uci$weekofmon)
```

### Capital BikeShare Data (2013-2017)

The five years of data amount to nearly 16 million rows combined.  Since UCI's data is aggregated on a daily scale, we do the same here and include variables found in the UCI dataset.  The time-consuming task here is encoding whether or not a date is a federal holiday, since some holidays change dates year to year. [ThoughtCo](https://www.thoughtco.com/public-holidays-in-the-united-states-3368327 ) has a helpful page describing the "calendar rules" of federal holidays.  By creating calendar-based columns like month, week of month, and day of the week, we can easily create a function that checks calendar conditions satisfying a federal holiday.

```{r}
library(data.table)
############## Now we import and clean the 2013-2017 data ##############
# data can be found from [here](https://www.capitalbikeshare.com/system-data)
# read as data.table for faster import
b <- list.files(full.names = TRUE)[1:20] %>% 
  lapply(fread) %>% 
  bind_rows

# convert to a dataframe to use dplyr functions
b <- data.frame(b)

# convert Start date to ymd:hms format
b$Start.date <- ymd_hms(b$Start.date, tz="US/Eastern")

# make a new column just with the date only
b$TripDate <- as.Date(b$Start.date, tz="US/Eastern")

# compute number of cyclists by date
cyclists <- b %>% 
  group_by(TripDate) %>% 
  summarise(casual = sum(Member.type == "Casual"),
            registered = sum(Member.type == "Member"),
            cnt = casual+registered) %>%
  mutate(weekday = wday(TripDate),
         mnth = month(TripDate),
         weekofmon = ifelse(between(day(TripDate),1,7), 1,
                            ifelse(between(day(TripDate),8,14), 2,
                                   ifelse(between(day(TripDate),15,21), 3, 4))))

## - Let's define federal holidays
# fixed holidays
holidays <- c(ymd("2013/01/01") + years(0:4), # New Years Day 
              ymd("2013/12/25") + years(0:4), # Christmas Day
              ymd("2013/11/11") + years(0:4), # Veterans Day
              ymd("2013/07/04") + years(0:4)) # Independence Day

# holidays that change dates; weekday ranges from 1-7, with 1 being Sunday
# info taken from https://www.thoughtco.com/public-holidays-in-the-united-states-3368327 
is_moving_hday <- function() {
  # Thanksgiving - fourth Thursday of November
  (cyclists$mnth == 11 & cyclists$weekofmon == 4 & cyclists$weekday == 5) |
    # MLK's Birthday - third Monday of January
    (cyclists$mnth == 1 & cyclists$weekofmon == 3 & cyclists$weekday == 2) |
    # George Washington's Birthday - third Monday of February
    (cyclists$mnth == 2 & cyclists$weekofmon == 3 & cyclists$weekday == 2) |
    # Memorial Day - fourth Monday of May
    (cyclists$mnth == 5 & cyclists$weekofmon == 4 & cyclists$weekday == 2) |
    # Labor Day - first Monday of September
    (cyclists$mnth == 9 & cyclists$weekofmon == 1 & cyclists$weekday == 2) |
    # Columbus Day - second Monday of October
    (cyclists$mnth == 10 & cyclists$weekofmon == 2 & cyclists$weekday == 2)  
}

# create a holiday column in cyclists data frame
cyclists <- cyclists %>%
  mutate(holiday = ifelse((is_moving_hday() | 
                            TripDate %in% holidays), 1, 0))
# convert calendar variables to factor variables
cyclists[, 5:8] <- lapply(cyclists[, 5:8], factor)
```

### NOAA Historical Weather Data (2011-2017)

CB doesn't include weather related variables like UCI does (the data contributor(s) added them separately), so we need to find this ourselves.  It's a fair assumption that temperature and precipitation affect ridership patterns so this is well worth finding.  The data is included in my repository but here's how you can get it yourself:

###### 1) go to https://www.ncdc.noaa.gov/cdo-web/datasets
###### 2) under Local Climatological Data, select "Search Tool"
###### 3) choose State, then Virginia
###### 4) navigate to Washington Reagan National Airport (closest weather station to DC)
###### 5) click Add to Cart
###### 6) go to your cart in the upper right hand side of the webpage; click View All Items
###### 7) choose LCD CSV, select dates 2011-01-01 to 2017-12-31, then click Continue
###### 8) fill in your email to get the data, then click Submit Order
###### The data will take a couple of minutes to be sent to your email

We're only concerned with the date, temperature, precipitation, snow, wind speed, and humidity.  Everything else can be discarded.

After examining the data we see December 31 info is missing for 2011, 2013 and 2014 so these are searched manually from [timeanddate](https://www.timeanddate.com/weather/usa/washington-dc/historic), averaging the four six-hour interval values for the aforementioned weather variables (data is given for 12am, 6am, 12pm, and 6pm).  Precipitation and snow values denoted as ```T``` (trace amounts) are replaced with 0.

```{r}
####### ---------- Include (dry-bulb) temperature, humidity, and precipitation data -------------###
# change file path to match yours
dc_weather <- read.csv("dcweather.csv")
# non-blank temps represent daily averages
dc_weather <- filter(dc_weather, !is.na(DailyAverageDryBulbTemperature))
# keep date, temp, and humidity columns - find their column numbers first
cols_keep <- match(c("DATE","DailyAverageDryBulbTemperature","DailyAverageRelativeHumidity",
                     "DailyAverageWindSpeed", "DailyPrecipitation", "DailySnowfall"), 
                   colnames(dc_weather))
# remove everything else
dc_weather <- dc_weather[, cols_keep]
# shorten/rename columns
colnames(dc_weather) <- c("TripDate", "temp", "hum", "windspeed", "prec", "snow")
# remove "T" and time values from date column; first convert to character
# equivalent to keeping elements 1-10 of the date string
dc_weather$TripDate <- as.character(dc_weather$TripDate)
dc_weather$TripDate <- substring(dc_weather$TripDate,1,10)
# now convert TripDate column to TripDate object
dc_weather$TripDate <- ymd(dc_weather$TripDate)
# quick check of weather data (using summary function) reveals
# T values in prec and snow; T means trace amounts, so we'll substitute 0 for T
# info taken from https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf
# first need to convert from factor to character - can't directly substitute from factor
dc_weather[, 5:6] <- apply(dc_weather[, 5:6], 2, function(x) as.character(x))
# now make the substitution
for (i in 5:6) {
  # if any value in column i equals T, replace with 0, otherwise keep original value
  dc_weather[, i] <- ifelse(dc_weather[, i] == "T", "0", dc_weather[, i])
}
# change prec and snow columns to numeric type
dc_weather[, 5:6] <- apply(dc_weather[, 5:6], 2, function(x) as.numeric(x))
# missing 12/31 date for 2011, 2013, and 2014. insert rows into dc_weather
# find row number Dec 30 for '11, '13, and '14.
dec_30s <- which(month(dc_weather$TripDate) == 12 & 
                 day(dc_weather$TripDate) == 30 &
                 year(dc_weather$TripDate) %in% c(2011,2013,2014))

# create a small dataframe to hold Dec 31 info
dec_31_info <- data.frame(TripDate = ymd(c("2011/12/31", "2013/12/31", "2014/12/31")),
                          temp = c(58,41,36),
                          hum = c(62,49,43),
                          windspeed = c(9,11,7),
                          prec = c(0,0,0),
                          snow = c(0,0,0)
)

# cut and paste together all the weather data
dc_weather <- rbind(dc_weather[1:364, ], # 1/1/2011 - 12/30/2011
                     dec_31_info[1,], # 12/31/2011
                     dc_weather[365:1094, ], # 1/1/2012 - 12/30/2013
                     dec_31_info[2, ], # 12/31/2013
                     dc_weather[1095:1458, ], # 1/1/2014 - 12/30/14
                     dec_31_info[3,], # 12/31/14
                     dc_weather[1459:2919, ] # everything else
                     )
```

### Combining all three datasets

Finally, we unite all three datasets to obtain 2011-2017 ridership data.  First, we replace the deleted UCI weather data with the 2011-2012 info from NOAA.  Then we update the CB data by attaching the NOAA weather data from 2013-2017 to it.  Finally, we horizontally stack the UCI and CB data (2011-2012 on top, 2013-2017 on the bottom) to make one complete dataset called ```bikes```.

```{r}
############## combining all three datasets ##############
# add prec and snow values for 2011 and 2012 to bikes_uci
# these are the substitutes for weathersit
bikes_uci <- dc_weather %>% 
  filter(between(TripDate, "2011/01/01", "2012/12/31")) %>%
  select(TripDate, prec, snow) %>%
  merge(bikes_uci, by="TripDate", all=T)

# merge dc_weather and cyclists -- this adds on the weather data to cyclists
bikes_temp <- merge(cyclists, dc_weather, by="TripDate")
# now we have all 2013-2017 data.  need to combine this with UCI 2011-2012 data
# first need to reorder columns of bikes_uci (or bike_temp)
cols_matched <- match(colnames(bikes_temp), colnames(bikes_uci))
bikes_uci <- bikes_uci[, cols_matched]
# now merge horizontally (stack UCI data on top of 2013-2017)
bikes <- bind_rows(bikes_uci, bikes_temp)
```

### Final Data Cleaning

Curiously, all humidity data from October 2013 is missing (as well as for four other dates).  

```{r}
bikes[which(is.na(bikes$hum)), c(1,4,9:11)]
```


Fortunately we can use past values to impute missing data.  The ACF plot below quantifies how strong current and past humidity values are correlated, such as ~ 0.5 correlation between a given day's value and yesterday's value.  

```{r}
Acf(bikes$hum, lag.max = 30, main="")
rect(xleft=0.5, ybottom=0, xright=1.5, ytop=0.53, border="red",lwd=2)
arrows(x0=4, y0=0.45, x1=2, y1=0.45, angle=20, col="red", length=.1)
text(x=3.7, y=0.45, labels = c("Lag 1"), pos=4, cex=.85)
title("Humidity Autocorrelations", line=1)
```

Using one day previous values of humidity would be great if we weren't missing 31 consecutive days!  However, if we assign a seasonality of 365 to humidity, decomposing the humidity data displays a strong annual pattern, as seen in the ```seasonal``` component below.

```{r}
# remove NAs from humidity column
hum_2 <- na.omit(bikes$hum)
# make into ts object
hum_2_ts <- ts(hum_2, frequency = 365)
stl(hum_2_ts, s.window = "periodic") %>% autoplot() + xlab("Time (Years)") + theme_minimal()
```

So we resort to using last year's date ( ~ 365 days) for each missing date since there's evidence of annual seasonality.  For example, we'll use humidity from 10/12/2012 as a substitute for 10/12/2013.  It's quite natural to ask what last year's value of something was if you're missing this year's data and have evidence of annual seasonality, as we do here.

```{r}
############## final data cleaning ##############
# find date of missing hum values and get the previous year's date
last_yrs <- bikes[which(is.na(bikes$hum)), ]$TripDate - years(1)
# get the associated humidity values
last_hum <- bikes %>% 
  filter(TripDate %in% last_yrs) %>%
  select(TripDate, hum)
# impute by first storing relevant row numbers
rows_to_impute <- which(is.na(bikes$hum))
bikes[rows_to_impute, 10] <- last_hum$hum
# to model trend (in MLR model), need to include time variable (Time)
# Time is simply the length of the time series (1:nrow(bikes))
bikes$Time <- 1:nrow(bikes) 
```

Lastly, we create a variable named ```Time``` which ranges from 1 to the total number of rows in ```bikes``` 2553, which implies Day 1 through 2553 of the dataset. Later on we'll use this to model trend in our regression models.

### Summary

There's a lot in this section! It reflects how many data science analyses go: 70-80% of time is spent data cleaning and feature engineering, and there's even more of the latter later in the modeling stage as we improve model performance.  For now let's restate what we've done so far.

* Replace categorical weather info in the UCI dataset with numerical values of snow and precipitation from NOAA
* Introduce week of month as an auxiliary feature, and use it alongside month of year to define federal holiday as a feature in the 2013-2017 data
* Impute missing weather data missing from NOAA
* Introduce a time variable to model trend (later in the analysis)

At this point we can (finally) explore the data!

## Exploratory Analysis

### Data Dictionary

Before exploring, here's a summary of all the variables and their meanings.

```{r}
library(kableExtra)
bike_variables <- data.frame(Variable = c("TripDate", "casual", "registered", "cnt", "weekday", "mnth", "weekofmon", "holiday", "temp", "hum", "windspeed", "prec", "snow", "Time"),
                             Description = c("Date of record", "Number of trips made by lower-tier membership plan riders", "Number of trips made by upper-tier membership plan riders", "Sum of the two previous columns", "Day of the week. 1 = Sunday", "Month of TripDate (1-12)", "Week of month of the TripDate (1-4)", "Is the TripDate a federal holiday? 1 = Yes", "Temperature in Fahrenheit", "Humidity expressed as a percent (maximum possible = 100)","Wind speed in miles per hour", "Amount of precipitation (rain) in inches","Amount of snowfall in inches", "Day Number (Day 1, Day 2, etc).  Used to model trend in regression models"))

bike_variables %>% kable() %>% kable_styling(full_width = F, bootstrap_options = "striped")
```

Our variable of interest to predict is ```cnt```.

### Overall Daily Ridership

The line plot below is interactive, so feel free to click and drag around your mouse to zoom in on any portion.  We see a clear positive trend and annual seasonality in ridership -- for example, people tend to ride more during the summer than winter months.

The tall upward peaks in 2014 and 2015 most likely reflect high turnout at the annual National Cherry Blossom Festival.

```{r eval=FALSE, fig.height=5, fig.width=8, include=FALSE}
library(plotly)
plot_ly(data = bikes,
        x = ~TripDate,
        y = ~cnt,
        hoverinfo = 'text',
        text = ~paste(TripDate,
                      '</br></br>', cnt),
        type="scatter",
        mode="lines") %>%
  layout(title = "Capital BikeShare Daily Trips (2011-2017)",
         xaxis = list(title = "", showgrid=F),
         yaxis = list(title = "Total Trips", showgrid=F))
```

### Exploring Weather Variables on Ridership

Below is a scatterplot matrix of continuous predictors (weather variables) plotted against each other and the response variable ```cnt``` (total number of trips).  Some comments:

* No significant multicollinearity exists between the weather variables.  This means regression coefficients for those variables should be stable
* Only temperature is strongly correlated with trip numbers
* Snowfall and precipitation negatively affect trips (as expected)

```{r}
pairs(bikes[, c(4,9:13)], lower.panel = NULL, col="steelblue")
```

### Ridership by Day of the Week

Trip patterns tend to fluctuate depending on day of week though not in a drastic way.  Nevertheless people tend to ride more in the middle of the week (Wednesdays in particular, beginning in 2015) and less during the weekends.

```{r}
library(numform)
library(gganimate)
library(magick)
dow_animation <- bikes %>%
  group_by(weekday, Year = as.integer(year(TripDate))) %>% 
  summarise(`Total Trips` = sum(cnt)) %>%
  ggplot(aes(weekday, `Total Trips`)) + 
  geom_bar(stat = "identity", fill="lightblue") +
  #geom_text(aes(label=paste0(round(`Total Trips`/1000), "K")), vjust=1.6, size=3.5)+
  scale_x_discrete(labels = c("1"="Sun", "2"="Mon", "3"="Tue","4"="Wed","5"="Thur","6"="Fri","7"="Sat"))+
  scale_y_continuous(label=ff_thous(digits=2))+
  theme(panel.background = element_blank(),
        axis.ticks = element_blank()) +
  labs(title="Total Trips by Day of Week: {frame_time}",
       x = "", y="Total Trips (in thousands)")+
  transition_time(Year)

dow_anim <- animate(dow_animation, nframes = 50, renderer = magick_renderer())
# save as gif
anim_save("dayofweek.gif", dow_anim)
```

![](/post/pvalues_intuitive_files/dayofweek.gif)