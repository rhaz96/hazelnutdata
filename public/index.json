[{
    "title": "hjj",
    "date": "",
    "description": "",
    "body": "\n\n\n\n",
    "ref": "/post/hjj/"
  },{
    "title": "Lines are NOT linear functions !?!",
    "date": "",
    "description": "",
    "body": "  Last updated: Jun 22, 2020  Wait, what? I know, you most likely did a double take on the title. After all, we were all probably taught in algebra or geometry class that lines are linear functions. So presumably \\(y = 2x+3\\) is supposed to be a linear function…except that it’s not. In fact equations of the form \\(y = mx+b\\) are not considered linear functions except under special conditions, which will be explained a bit later. All this raises the question: what are linear functions?\n Linear functions, defined At the heart of linear algebra are linear transformations, like matrix-vector products. Such transformations are really just linear functions - we use the word function in algebra and transformation in linear algebra, but they’re all one and the same concept. Ivan Savov’s excellent and humorous book on linear algebra (which you can find here) provides the definition:\n A function f is linear if it satisfies the equation \\[f(\\alpha x_1 + \\beta x_2) = \\alpha f(x_1) + \\beta f(x_2)\\]\n whera \\(\\alpha\\) and \\(\\beta\\) are both scalar constants. MIT provides the same definition albeit with slightly different notation.\nThe definition implies that evaluating a linear function with some linear combination of inputs \\(x_1\\) and \\(x_2\\) is the same as taking a linear combination of the outputs \\(f(x_1)\\) and \\(f(x_2)\\).\n An example of violation We can show how a line like \\(f(x) = \\frac{1}{2}x+5\\), absent certain limitations on \\(\\alpha\\) or \\(\\beta\\), violates the definition of linearity. Let \\(x_1\\) = 3 and \\(x_2\\) = 6 be inputs, and let \\(\\alpha\\) = 4 and \\(\\beta\\) = 2 be scalar constants to form a linear combination of the inputs. Then \\(f(x)\\) would be a linear function if \\(f(4 x_1 + 2 x_2) = 4 f(x_1) + 2 f(x_2)\\). Is it? Let’s see.\n\\(f(4 x_1 + 2 x_2) = \\frac{1}{2}(4*3 + 2*6)+5 = 17\\)\n\\(4 f(x_1) + 2 f(x_2) = 4[\\frac{1}{2}(3) + 5] + 2[\\frac{1}{2}(6) + 5] = 42\\)\nClearly 17 \\(\\neq\\) 42 so the function as it stands is not a linear function. But we’ll next consider under what circumstances such a line as this can be considered a linear function.\n When lines are linear functions Neither Savov nor the MIT page linked earlier show and/or derive the condition for when a generic line \\(y = mx+b\\) can be considered a linear function. It turns out that when \\(\\beta\\) = \\(1-\\alpha\\) it’s indeed a linear function.\n If \\(\\beta\\) = \\(1-\\alpha\\), then a function of the form \\(f(x) = mx+b\\) is a linear function\n To prove this we need to show \\(f(\\alpha x_1 + \\beta x_2) = \\alpha f(x_1) + \\beta f(x_2)\\). Let’s take the left hand side first:\n\\(f(\\alpha x_1 + \\beta x_2) = m(\\alpha x_1 + \\beta x_2) + b = \\alpha m x_1 + \\beta mx_2 + b\\)\nNow the RHS:\n\\(\\alpha f(x_1) = \\alpha (mx_1+b) = \\alpha mx_1 + \\alpha b\\)\n\\(\\beta f(x_2) = \\beta (mx_2+b) = \\beta mx_2 + \\beta b\\)\nPutting it altogther:\n\\(\\alpha m x_1 + \\beta mx_2 + b = \\alpha mx_1 + \\alpha b + \\beta mx_2 + \\beta b\\)\nAfter cancelling like terms we’re left with:\n\\(b = \\alpha b + \\beta b\\)\nFactoring out b from the RHS above and simplifying we get:\n\\(b = b(\\alpha + \\beta)\\) \\(\\rightarrow\\) \\(1 = \\alpha + \\beta\\) \\(\\rightarrow\\) \\(\\beta\\ = 1-\\alpha\\)\nSo we’ve just shown the limiting condition that ensures a line is a linear function.\n Cancelling the violation Let’s return to the function we considered earlier: \\(f(x)=\\frac{1}{2}x + 5\\). If we keep the original value \\(\\alpha\\) = 4 but modify \\(\\beta\\) to be \\(1-4 = -3\\), we’ll see the function now qualifies as a linear function.\n\\(f(4 x_1 - 3 x_2) = \\frac{1}{2}(4*3 - 3*6)+5 = 2\\)\n\\(4 f(x_1) - 3 f(x_2) = 4[\\frac{1}{2}(3) + 5] - 3[\\frac{1}{2}(6) + 5] = 2\\)\nPerfect!\n Wrapping up I was surprised to learn the generic equation of lines don’t actually qualify as linear functions. The only lines that do qualify to be linear functions are those that go through the origin; that is, their equation is \\(f(x) = mx\\). So \\(b=0\\) — you can verify this by going back to the proof above and subsituting \\(0\\) for \\(b\\). All values of \\(\\alpha\\) and \\(\\beta\\) will satisfy the linear function requirements. If \\(b\\) \\(\\neq\\) \\(0\\) then we must restrict \\(\\beta\\) to be equal to \\(1-\\alpha\\). Equations of the form \\(f(x) = mx+b\\) with \\(b\\) \\(\\neq\\) \\(0\\) are best thought of as linear equations and not strictly as linear functions.\nThis is just one of the nuggets from the gold mine of linear algebra. There’s much more that you and I can uncover by learning this fascinating subject!\n ",
    "ref": "/post/lines-not-linear-functions/"
  },{
    "title": "Comparing Interaction Terms in Log-Log Regression Models",
    "date": "",
    "description": "",
    "body": "  p.caption { font-size: 0.8em; }   The venerable House Price Competition on Kaggle  \nIn the graduate statistics class I’ve TA’ed, we explore a range of regression concepts, both theory and practice. This includes linear and polynomial regression, as well as transformations to align data to certain regression modeling assumptions. At the end of the course, students complete form groups to work on a project…the Kaggle housing prices competition on Ames, Iowa - perhaps the most well known and ongoing competition. This introduces students to real-world modeling and the ‘Kagglesphere’. Yes, we know linear regression isn’t the winning technique for this competition, but it’s simply an exercise in modeling. Also, the students in the best-scoring group gets 3 points extra credit on the final exam. That’s the real competition!\nBefore delving into building a full model on the entire dataset, students are given an “appetizer” (ok, it’s not actually called that but that’s how I refer to it) where they focus on predicting house selling prices for just three neighborhoods — Brookside, Edwards, and North Ames — using a home’s living area as a predictor. By using this smaller subset it’s easier to explore the effects of certain regression techniques, and interpret model output. After all, linear regression is a highly interpretable machine learning algorithm (yes, linear regression is machine learning) and should thus be exploited whenever possible.\nNow, just because linear regression models are interpretable doesn’t mean they can’t become complex. Complexity often arises from combining use of interaction terms and logarithmic transforms. In the appetizer, interaction terms allow the slope of each neighborhood’s selling price to differ.\nSuppose increasing a house’s living area is generally associated with an increase in a house’s price. An interaction term stipulates that the price hike per square foot increase of living area will be different for each of the three neighborhoods. A 1\\({ft^2}\\) increase in living area may be associated with a $50 increase in selling price for North Ames homes, a $30 increase for Brookside homes, and a $65 increase for Edwards homes.\nThrowing in a log transform makes things a bit more tricky to interpret. Small wonder my textbook admonishes\n “it is often difficult to interpret individual coefficients in an interaction model.”1\n Several students came to me asking how to interpret interaction terms in a log-log model. I created a PowerPoint that became the basis for this blog post!\nLog-Log Model with Interaction Terms What would such a model look like for the appetizer model? Here’s some SAS output with the relevant portions highlighted\n  Notice that North Ames (NAmes) is considered the reference category. There’s no particular reason North Ames was chosen. We’ll focus on comparing Brookside home prices to North Ames prices. Our goal is to interpret logSqFoot*Neighborho BrkSide, the interaction term for Brookside homes.\nLet’s first begin by writing out the equation for those neighborhoods (I’m rounding all values to the nearest hundredth).\nBrookside \\(ln(Price) = 8.49+0.47*ln(sqft)-2.58+0.35*ln(sqft) = 5.91+0.82*ln(sqft)\\)\n North Ames \\(ln(Price) = 8.49+0.47*ln(sqft)\\)\nNotice how both the response and explanatory variables are (natural) log transformed.\n  Individual Interpretation Generally speaking, an equation of the form \\(ln(Y) = \\beta_0 + \\beta_1*ln(X)\\) has the following interpretation:\n a doubling of X is associated with a \\(2^{\\beta_1}\\) multiplicative increase in the median of Y\n We can show this with the following derivation (taken from my class notes)\nApplying this to our two equations above\nBrookside Interpretation A doubling in living area is associated with a \\(2^{0.82}\\) multiplicative increase (or 77% increase) in median Brookside home prices.\n North Ames Interpretation A doubling in living area is associated with a \\(2^{0.47}\\) multiplicative increase (or 39% increase) in median North Ames home prices.\n  Interaction Term Interpretation Notice the interaction term logSqFoot*Neighborho BrkSide is 0.35, which is the difference between 0.82 and 0.47. This tells us 0.35 is the adjustment in the slope of home selling prices for Brookside homes, relative to North Ames homes. We can, however, give a more precise interpretation of the interaction term by noting\n\\(\\frac{Multiplicative\\,Increase\\,BkSide}{Multiplicative\\,Increase\\,NAmes} = \\frac{2^{\\,0.82}}{2^{\\,0.47}} = 2^{\\,0.82-0.47} = 2^{\\,0.35}\\)\nThis implies\n\\(Multiplicative\\,Increase\\,BkSide = 2^{0.35}*(Multiplicative\\,Increase\\,NAmes)\\)\nSo we can then say:\n For a doubling in living area, the multiplicative increase in median sale price for Brookside homes is \\(2^{0.35}\\) (= 1.27) times greater than the multiplicative increase in median sales prices for North Ames homes, holding all other variables constant\n Not a straightforward interpretation!\n Motivating an alternate interpretation Another way to interpret \\(\\beta_1\\) in the equation \\(ln(Y) = \\beta_0 + \\beta_1*ln(X)\\) is\n A 1% increase in X is associated with a \\(\\beta_1\\)% increase in Y, holding all other variables constant\n I’ll point you to this and this Stack Exchange post on why that makes sense. Those resources however don’t provide an interpretation of log-log interaction terms using percents. So let’s do that now.\nBrookside Interpretation (with Percents) A 1% increase in living area is associated with a 0.82% increase in median Brookside home price, holding all other variables constant\n North Ames Interpretation (with Percents) A 1% increase in living area is associated with a 0.47% increase in median North Ames home price, holding all other variables constant\nThis implies the following interpretation for logSqFoot*Neighborho BrkSide\n There’s a 0.35 percentage point difference in median sales price between Brookside and North Ames homes for a 1% increase in living area, holding all other variables constant.\n Note that going from 0.47% to 0.82% is a difference of 0.35 percentage points (not percent). In terms of a percent, it’s a \\(\\frac{0.82-0.47}{0.47}*100\\) = 74.47% increase.\n  Interaction Interpretation Template We can summarize both interpretations by providing a general template, where the terms in curly braces are replaced with the relevant variable names\n“Doubling” Interpretation  For a doubling in {x-variable}, the multiplicative increase in median {response variable} for {category} is \\(2^{interaction\\,term\\,value}\\) times greater than the multiplicative increase in median {response variable} for {reference category}, holding all other variables constant.\n  “Percentage Point” Interpretation  There’s a {interaction term value} percentage point difference in median {response variable} between {category} and {reference category} for a 1% increase in {x-variable}, holding all other variables constant.\n   Wrapping Up Perhaps it’s best to follow my textbook’s advice and avoid interpreting complex interaction terms. But if you’re a curious soul seeking meaning (of interaction terms), I hope this satisfies your curiosity.\n  The Statistical Sleuth, 3rd Edition (2013), Fred Ramsey and Daniel Schafer, p.250↩\n   ",
    "ref": "/post/log-log-interaction/"
  },{
    "title": "Visualizing a Portuguese retail bank's marketing efforts",
    "date": "",
    "description": "",
    "body": "  Business Understanding Data Understanding Data Dictionary Verifying Data Quality Simple Statistics Visualize Attributes Explore Joint Attributes Explore Attributes and Prediction Class Extra features  Extras Rejecting the bank: what are the odds?    import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # using a relative path would probably be better! bankingDF = pd.read_csv(\u0026quot;/Users/roberthazell/Desktop/SMU/ML1/bank-additional-full.csv\u0026quot;) Business Understanding The Bank Marketing Data Set from the UCI Machine Learning Repository was selected to be analyzed by our group. The data involves information obtained from direct telemarketing campaigns from a Portuguese banking institution. The data contains 41,188 observations with 20 attributes and was collected between May 2008 to November 2010.\nOur objective is to build a customer conversion model that predicts the likelihood a customer converts after seeing a bank term deposit offer. A cash investment, which will be held at the bank, is invested at an agreed rate of interest over a fixed period of time. Characteristics of a customer such as age, education, type of job, and many others will be analyzed to determine if they are factors determining conversion.\nA good prediction algorithm would establish a relationship or correlation between specific attributes with the probability of whether a bank customer would subscribe to a bank term deposit. We will assess the qualities of our models using a confusion matrix. With the results from our analysis, the attributes that are strongly correlated to a customer subscribing to a bank term deposit would be emphasized in future telemarketing campaigns.\n Data Understanding Data Dictionary The description for the 20 data attributes are taken from: https://archive.ics.uci.edu/ml/datasets/bank+marketing, but the complete list of features have been witheld from the dataset.\n.tg {border-collapse:collapse;border-spacing:0;border-color:#ccc;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#fff;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#f0f0f0;} .tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}   Attribute  Data Type  Description  Examples    age  numeric  The age of the customer in years  26, 45, 50    job  categorical  The type of job of the customer  admin, blue-collar, entrepreneur, housemaid, management, retired, self-employed, services, student,technician, unemployed, unknown    marital  categorical  The marital status of the customer  divorced, married, single, unknown    education  categorical  The education level of the customer  basic.4y, basic.6y, basic.9y, high.school, illiterate, professional.course, university.degree, unknown    default  categorical  If the customer has their credit in default  yes, no    housing  categorical  If the customer has a housing loan  yes, no    loan  categorical  If the customer has a personal loan  yes, no    contact  categorical  Customer contact communication type  cellular, telephone    month  categorical  Last contact month with the customer  jan, feb, mar, …, nov, dec    day_of_week  categorical  Last contact day of the week with the customer (note that the bank is only open during weekdays)  mon, tue, wed, thu, fri    duration  numeric  Duration of the last contact with customer in seconds  20, 30, 60    campaign  numeric  The number of contacts performed during the campaign for the customer  3, 7, 15    pdays  numeric  Number of days passed since the customer was last contacted (999 means customer was never contacted)  20, 30, 999, etc.    previous  numeric  Number of contacts made with customer before this campaign  20, 30, 1000, etc.    poutcome  categorical  Outcome of the previous marketing campaign  failure, nonexistent, success    emp.var.rate  numeric  Employment variation rate - quarterly indicator  -1.2, 1.1, 2.3    cons.price.idx  numeric  Consumer price index - monthly indicator  91.21, 93.33, 94.32    cons.conf.idx  numeric  Consumer confidence index - monthly indicator  -50.8, -36.6, -26.9    euribor3m  numeric  Interest rate where a selection of European banks lend one another funds in euros where loans have a maturity of 3 months  1.048, 4.857, 3.569    nr.employed  numeric  Quarterly average of the total number of employed citizens  4198, 5191, 5325    y  categorical  Has the client subscribed to a term deposit? (the product)  yes, no     Verifying Data Quality The quality of the data initially appears relatively good as there are no missing or NA values. However, further reading into the documentation reveals several missing attribute values, all for categorical variables. These are encoded as “unknown”. We’ll need to decide on either treating these “unknown” values as a possible class label, delete or impute these values. The attributes that contain the “unknown” value are:\n job (330 instances) marital (80 instances) education (1731 instances) default (8597 instances) housing (990 instances) loan (990 instances)  For now in our exploratory data analysis we’ll keep ‘unknown’ values as a class label, but will need to re-evaluate this value in future analysis.\nThere are 12 duplicate observations, which accounts for about 0.1% of the data. However, without an attribute to indicate if each observation is unique like ‘CustomerId’, it is very possible that the duplicate observations are legitmate observations that are unrelated to the other duplicates.\nThere are outliers in several attributes. For age the majority of the customers fall between the ages of 17 and 60, with 10 customers over the age of 90. The campaign attribute, which measures how often a customer is contacted, also has some extreme values where 8 customers were contacted over 40 times during a marketing campaign. Visualization of these and other features are presented a little later. Future analysis will need to be done with and without outliers to determine any possible improvement in model adequacy.\n Simple Statistics The table below displays the simple statistics for the numeric values of the data set.\nWe see the average bank customer is around 40 years old and contacted 2 to 3 times during the marketing campaign, but rarely contacted before the marketing campaign as deduced from the means of pdays and previous attributes, which measure the number of days after the customer was last contacted from a previous campaign and the number of times the customer was contacted before the current campaign respectively. As one would expect, the targeted customers are at an age where investing makes sense. Customers in the lower age range (late teens to early 30’s) are less likely to invest their money as they may not be planning for the future or may not have the disposable income to invest.\nbankingDF.describe()  age duration ... euribor3m nr.employed count 41188.00000 41188.000000 ... 41188.000000 41188.000000 mean 40.02406 258.285010 ... 3.621291 5167.035911 std 10.42125 259.279249 ... 1.734447 72.251528 min 17.00000 0.000000 ... 0.634000 4963.600000 25% 32.00000 102.000000 ... 1.344000 5099.100000 50% 38.00000 180.000000 ... 4.857000 5191.000000 75% 47.00000 319.000000 ... 4.961000 5228.100000 max 98.00000 4918.000000 ... 5.045000 5228.100000 [8 rows x 10 columns] Strictly viewing the results of if a customer subscribed or not, the percentage of customers who did subscribe is a little over 11%.\n# The percentage of individuals that subscribed to the bank len(bankingDF[bankingDF.y==\u0026#39;yes\u0026#39;])/len(bankingDF)*100.0 11.265417111780131 In the data set, over 60% of the customers are married as opposed to being single, divorced or a small percentage being unknown. This makes sense as married customers are most likely to invest for the future whether it being to purchase a home or to pay for their children’s college education.\n# Marital status basic statistics bankingDF.marital.describe() count 41188 unique 4 top married freq 24928 Name: marital, dtype: object  Visualize Attributes Age of Potential Customers # histogram of age ax = sns.distplot(bankingDF[\u0026quot;age\u0026quot;]) ax.grid(False) plt.tick_params(labelsize=12) plt.show() There is some right skew in terms of the age range the bank targets, with the majority of customers targeted in their 30s-40s. This makes sense as those in that age bracket would more likely have money to spend on long-term deposits given a stable income. Individuals past 60 years old are not considered, most likely since such are already retired; they should have made long-term deposits decades before!\n Job Categories fig, ax = plt.subplots(figsize=(20,10)) job_totals = bankingDF[\u0026quot;job\u0026quot;].value_counts() job_totals = pd.DataFrame({\u0026quot;Job Type\u0026quot;: job_totals.index, \u0026quot;Total\u0026quot;: job_totals.values}) ax = sns.barplot(x = \u0026quot;Total\u0026quot;, y=\u0026quot;Job Type\u0026quot;, data=job_totals, palette=\u0026quot;Blues_d\u0026quot;) ax.grid(False) plt.tick_params(labelsize=20) _=plt.xlabel(\u0026quot;Total Prospects\u0026quot;, fontsize = 22) _=plt.ylabel(\u0026quot;Job Type\u0026quot;, fontsize = 22) plt.show() The bank tends to heavily profile blue-collar workers in that three of the top four job types are in the blue-collar sector. The number one category – admin – is white-collar.\n How frequent a prospect is called (current campaign) _=plt.boxplot(bankingDF[\u0026quot;campaign\u0026quot;], vert=False) _=plt.xlabel(\u0026quot;Number of times contacted\u0026quot;, fontsize=20) plt.grid(False) fig = plt.gcf() a = plt.gca() _=a.axes.get_yaxis().set_ticks([]) fig.set_size_inches(18.5, 8) plt.tick_params(labelsize=18) plt.show() On average the bank has been in contact less than 5 times for any given customer, but several were constantly targeted. Perhaps these are highly promising but wavering customers.\n When are calls made? fig, ax = plt.subplots(figsize=(15,10)) # count number of calls by day call_day = bankingDF[\u0026quot;day_of_week\u0026quot;].value_counts() # convert to dataframe call_day = pd.DataFrame({\u0026quot;Day\u0026quot;: call_day.index, \u0026quot;Total\u0026quot;: call_day.values}) # plot barplot of counts ax = sns.barplot(call_day[\u0026quot;Day\u0026quot;], call_day[\u0026quot;Total\u0026quot;], palette=\u0026quot;Purples_d\u0026quot;) ax.grid(False) _=plt.xlabel(\u0026#39;\u0026#39;) _=plt.ylabel(\u0026quot;Calls Made\u0026quot;, fontsize=20) plt.tick_params(labelsize=18) plt.show() In terms of follow-up call days, bankers show some preference, most following up on Thursday and least on Fridays, but these differences don’t seem significant.\n Time elapsed since a prospect was previously called # plotting how long it took for a prospect to receive # another campaign call # remove people who were contacted for the first time bank_pdays_filtered = bankingDF[bankingDF[\u0026quot;pdays\u0026quot;] != 999] # plot data this way to change x-axis label from default ax = sns.violinplot(x=\u0026quot;pdays\u0026quot;, data = bank_pdays_filtered, color=\u0026quot;turquoise\u0026quot;) #_=ax.set(xlabel = \u0026quot;Days since last contact\u0026quot;) ax.grid(False) plt.tick_params(labelsize = 18) _=plt.xlabel(\u0026quot;Days since last contact\u0026quot;, fontsize=19) plt.show() We see evidence of bimodality and right-skew from this violinplot, aspects not clearly seen in the inscribed boxplot.\nBankers followed up with them roughly between 3 and 6 days after their latest call for most people in this data. The two peaks are perhaps indicative of early stage sale-pitching by the banks where communication is frequent. For those in late-stage or completed negotiations, or people who constantly ignore campaign calls, the right skew probably reflects such cases.\n Education level of prospects fig, ax = plt.subplots(figsize=(10,6)) education_totals = bankingDF[\u0026quot;education\u0026quot;].value_counts() education_totals = pd.DataFrame({\u0026quot;Education Type\u0026quot;: education_totals.index, \u0026quot;Total\u0026quot;: education_totals.values}) my_range=range(1,len(education_totals.index)+1) _=plt.hlines(y=my_range, xmin=0, xmax=education_totals[\u0026#39;Total\u0026#39;], color=\u0026#39;skyblue\u0026#39;) _=plt.plot(education_totals[\u0026#39;Total\u0026#39;], my_range, \u0026quot;o\u0026quot;) _=plt.yticks(my_range, education_totals[\u0026#39;Education Type\u0026#39;]) _=plt.grid(False) _=plt.tick_params(labelsize=\u0026quot;small\u0026quot;) _=plt.show() Code for lollipop chart adapted from here. As you’d expect the bank tends to target those who have, at the very least, completed high school. Greatest preference is shown towards college educated individuals. Again, this makes sense since they tend to earn more money relative to other education groups, making them likelier to deposit money.\n  Explore Joint Attributes Job and Education _=plt.subplots(figsize=(10,8)) _=sns.heatmap(pd.crosstab(bankingDF[\u0026quot;education\u0026quot;],bankingDF[\u0026quot;job\u0026quot;]), annot=True, fmt=\u0026quot;g\u0026quot;) plt.tick_params(rotation=45, labelsize=7) _=plt.ylabel(\u0026quot;Education\u0026quot;) _=plt.xlabel(\u0026quot;Job\u0026quot;) plt.show() We can visualize a cross tabulation between education level and job status. There isn’t much to be said for illiterate folks, but for those with unknown education tend to work blue-collar, admin, and technician type jobs. The most blue collar employees are those with basic.4y, basic.6y, and basic.9y educations. It’s not clear what these categories represent. College educated individuals work predominantly as admins, in management, or as technicians. High school grads also work in large proportion within admin, but also in the services industry.\n Consumer Confidence and Consumer Price Indexes ax = sns.lmplot( x=\u0026quot;cons.price.idx\u0026quot;, y=\u0026quot;cons.conf.idx\u0026quot;, data=bankingDF, fit_reg=True, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;}) plt.grid(False) _=plt.xlabel(\u0026quot;Price Index\u0026quot;) _=plt.ylabel(\u0026quot;Confidence Index\u0026quot;) plt.rcParams.update({\u0026#39;font.size\u0026#39;: 12}) plt.show() Almost no correlation is present between consumer confidence and the price index. That being said, the price index is relatively consistent in this data, ranging from ~ 92 to ~ 95, so it’s unsurprising to see a flat relationship between these variables (the correlation is ~ 0.06). Consumer confidence was negative in Portugal throughout 2014 (when this data was taken) and has been that way for nearly the past decade, as can be seen here. As an aside, that the correlation between price index and consumer confidence is slightly positive is mildly interesting.\n Month and Day of Week of Contact pd.crosstab(bankingDF[\u0026quot;day_of_week\u0026quot;],bankingDF[\u0026quot;month\u0026quot;]) month apr aug dec jul jun mar may nov oct sep day_of_week fri 610 1070 24 1012 1147 94 2858 755 142 115 mon 702 1222 53 1516 1251 143 2642 766 129 90 thu 768 1347 45 1672 967 99 2537 903 163 122 tue 252 1296 25 1517 970 140 2809 814 149 118 wed 300 1243 35 1457 983 70 2923 863 135 125 fig, ax = plt.subplots(figsize=(15,10)) day_and_month = pd.crosstab(bankingDF[\u0026quot;day_of_week\u0026quot;],bankingDF[\u0026quot;month\u0026quot;]) day_and_month = day_and_month.reindex([\u0026#39;mar\u0026#39;, \u0026#39;apr\u0026#39;, \u0026#39;may\u0026#39;, \u0026#39;jun\u0026#39;, \u0026#39;jul\u0026#39;, \u0026#39;aug\u0026#39;, \u0026#39;sep\u0026#39;,\u0026#39;oct\u0026#39;, \u0026#39;nov\u0026#39;, \u0026#39;dec\u0026#39;], axis=1).reindex([\u0026quot;mon\u0026quot;,\u0026quot;tue\u0026quot;,\u0026quot;wed\u0026quot;,\u0026quot;thu\u0026quot;,\u0026quot;fri\u0026quot;]) _=sns.heatmap(day_and_month, annot=True, fmt=\u0026quot;d\u0026quot;, cmap=\u0026quot;YlGnBu\u0026quot;, ax=ax,annot_kws={\u0026quot;size\u0026quot;:16}) _=plt.xlabel(\u0026quot;\u0026quot;) _=plt.ylabel(\u0026quot;\u0026quot;) _=plt.tick_params(labelsize=15) plt.show() The most popular months for calling are the late spring and summer months: May through August. The least popular months were in the fall and winter (September, October, and December), although November marks an uptick in campaign calling. There is no data for January or February, implying the latest campaign began in March.\n Timing of return call depending on previous campaign outcome # filter out the first-time calls outcome = bankingDF.loc[bankingDF[\u0026quot;poutcome\u0026quot;].isin([\u0026quot;failure\u0026quot;, \u0026quot;success\u0026quot;])] # find mean time elpased from previous campaign call outcome_tab = outcome[outcome[\u0026quot;pdays\u0026quot;] != 999].groupby(\u0026quot;poutcome\u0026quot;).agg({\u0026quot;pdays\u0026quot;:\u0026quot;mean\u0026quot;}) outcome_tab.index.names = [\u0026quot;Prev. Call Outcome\u0026quot;] outcome_tab.rename(columns = {\u0026quot;pdays\u0026quot;: \u0026quot;Avg. Days Until Next Call\u0026quot;})  Avg. Days Until Next Call Prev. Call Outcome failure 10.140845 success 5.587764 On average, if a previous campaign call failed, the bank would wait about twice as long to recall the prospect compared to recall time for previous campaign calls that succeeded.\n  Explore Attributes and Prediction Class As is often the case, this class is imbalanced. People tend to reject the bank’s offers, so oversampling those who didn’t reject (those who do subscribe to a term deposit) is needed in the future.\nEducation and Subscribers # all those who subscribed, grouped by education accepted = bankingDF[bankingDF[\u0026quot;y\u0026quot;] == \u0026quot;yes\u0026quot;].groupby(\u0026quot;education\u0026quot;).agg({\u0026quot;y\u0026quot;:\u0026quot;count\u0026quot;}).reset_index() accepted = accepted.sort_values(\u0026#39;y\u0026#39;) # lollipop plot my_range=range(1,len(accepted.index)+1) sns.set_style(\u0026#39;dark\u0026#39;) fig, ax = plt.subplots(figsize=(8,6)) _=plt.hlines(y=my_range, xmin=0, xmax=accepted[\u0026#39;y\u0026#39;], color=\u0026#39;maroon\u0026#39;) _=plt.plot(accepted[\u0026#39;y\u0026#39;], my_range, \u0026quot;o\u0026quot;, color = \u0026#39;maroon\u0026#39;) _=plt.yticks(my_range, accepted[\u0026#39;education\u0026#39;], rotation=45) _=plt.xlabel(\u0026#39;Contacts who accepted bank offer\u0026#39;) _=plt.tick_params(labelsize=10) _=plt.ylabel(\u0026#39;Education Level\u0026#39;) The majority who sign up with the bank are college educated, but the pattern above is almost identical for those who reject the bank. This suggests the need to examine prevalence rates across education levels, as will be seen later.\n Call Duration and Outcome (Non First time calls) The vast majority of bank calls are to first time contacts, so we’ll like to see the call distributions for non-first timers.\nfirst_timers = len(bankingDF[bankingDF[\u0026quot;pdays\u0026quot;] == 999]) / bankingDF.shape[0] print(\u0026quot;Proportion of first time calls: \u0026quot;, round(first_timers, 3)) Proportion of first time calls: 0.963 success = bankingDF[(bankingDF[\u0026#39;y\u0026#39;] == \u0026quot;yes\u0026quot;) \u0026amp; (bankingDF[\u0026#39;pdays\u0026#39;]!=999)] fail = bankingDF[(bankingDF[\u0026#39;y\u0026#39;] == \u0026quot;no\u0026quot;) \u0026amp; (bankingDF[\u0026#39;pdays\u0026#39;]!=999)] fig, ax = plt.subplots(figsize = (12,8)) for a in [success, fail]: _=sns.distplot(a[\u0026quot;duration\u0026quot;], bins=range(1, 3600, 20), ax=ax, kde=False) _=ax.set_xlim([0, 4000]) _=fig.legend(labels = [\u0026quot;Subscribed\u0026quot;,\u0026quot;Rejected\u0026quot;], fontsize=12) _=plt.xlabel(\u0026quot;Duration of Last Phone Call (Seconds)\u0026quot;, fontsize=14) _=plt.grid(False) _=plt.tick_params(labelsize=13) _=plt.show() As is pointed out call duration isn’t known before the bank makes a call, and the outcome of the call is known immediately after a call. So duration can’t be used as a predictor when building a model. Nevertheless, for those who’ve been contacted before, bank-rejecters spend on average less time engaging with the bank agent. The distribution of talk time is right-skewed regardless of call outcome. When analyzing the response and duration it needs to be done in a retrospective manner.\n Day of Week and Subscriptions fig, ax = plt.subplots(figsize=(5,5)) _=sns.heatmap(pd.crosstab(bankingDF[\u0026quot;day_of_week\u0026quot;],bankingDF[bankingDF[\u0026#39;y\u0026#39;] == \u0026quot;yes\u0026quot;][\u0026quot;y\u0026quot;]).reindex([\u0026quot;mon\u0026quot;,\u0026quot;tue\u0026quot;,\u0026quot;wed\u0026quot;,\u0026quot;thu\u0026quot;,\u0026quot;fri\u0026quot;]), annot=True,ax=ax, fmt=\u0026quot;g\u0026quot;) _=ax.set_xlabel(\u0026quot;\u0026quot;) _=ax.set_ylabel(\u0026quot;\u0026quot;) plt.rcParams.update({\u0026#39;font.size\u0026#39;: 18}) plt.show() The most successes come on Thursdays, followed by a near tie between Tuesday and Wednesday. It seems like the beginning and end of the week are the least favorable days to call.\n Job Profession of those who Subscribe fig, ax = plt.subplots(figsize=(10,6)) # all those who subscribed, grouped by education accepted = bankingDF[bankingDF[\u0026quot;y\u0026quot;] == \u0026quot;yes\u0026quot;].groupby(\u0026quot;job\u0026quot;).agg({\u0026quot;y\u0026quot;:\u0026quot;count\u0026quot;}).reset_index() accepted = accepted.sort_values(\u0026#39;y\u0026#39;) # lollipop plot my_range=range(1,len(accepted.index)+1) sns.set_style(\u0026#39;dark\u0026#39;) _=plt.hlines(y=my_range, xmin=0, xmax=accepted[\u0026#39;y\u0026#39;]) _=plt.plot(accepted[\u0026#39;y\u0026#39;], my_range, \u0026quot;o\u0026quot;, color=\u0026quot;black\u0026quot;) _=plt.yticks(my_range, accepted[\u0026#39;job\u0026#39;]) _=plt.tick_params(axis=\u0026quot;y\u0026quot;, labelsize=8, rotation=30) _=plt.tick_params(axis=\u0026quot;x\u0026quot;, labelsize=11) _=plt.xlabel(\u0026#39;Contacts who accepted bank offer\u0026#39;, fontsize=12) _=plt.ylabel(\u0026#39;Job Type\u0026#39;, fontsize=12) Interestingly, retired folks were one of the least targeted groups (look back at the age histogram) that ended up subscribing quite often.\n Prevalance Rates by Job Category The plot immediately above gives the raw values of subscribers by job type. We also know the overall prevalence rate is ~ 11%. Perhaps there’s quite a bit of imbalance between education or job positions so we’ll see if prevalence rates differ by category.\n_=plt.subplots(figsize=(8,6)) # those who accept by job category job_yes = (bankingDF[bankingDF[\u0026quot;y\u0026quot;] == \u0026quot;yes\u0026quot;] .groupby(\u0026#39;job\u0026#39;) .agg({\u0026#39;y\u0026#39;:\u0026#39;count\u0026#39;}) .rename(columns = {\u0026#39;y\u0026#39;:\u0026#39;Subscribed\u0026#39;}) .reset_index()) # total number of people in each job category # regardless of subscription status all_job = (bankingDF .groupby(\u0026#39;job\u0026#39;) .agg({\u0026#39;y\u0026#39;:\u0026#39;count\u0026#39;}) .rename(columns = {\u0026#39;y\u0026#39;:\u0026#39;Job Total\u0026#39;}) .reset_index()) # calculate prevalence rates job_yes[\u0026#39;Job Total\u0026#39;] = all_job[\u0026quot;Job Total\u0026quot;] job_yes[\u0026quot;Prevalence Rate\u0026quot;] = job_yes[\u0026quot;Subscribed\u0026quot;]/job_yes[\u0026quot;Job Total\u0026quot;] job_yes = job_yes.sort_values(\u0026quot;Prevalence Rate\u0026quot;, ascending=False) _= sns.barplot(job_yes[\u0026quot;job\u0026quot;], job_yes[\u0026quot;Prevalence Rate\u0026quot;], palette=sns.cubehelix_palette(12, reverse=True), ) _=plt.xlabel(\u0026quot;\u0026quot;) _=plt.tick_params(axis=\u0026quot;x\u0026quot;, labelsize=8, rotation=30) _=plt.tick_params(axis=\u0026quot;y\u0026quot;, labelsize = 11) _=plt.ylabel(\u0026quot;Prevalence Rate\u0026quot;, fontsize = 12) plt.show() job_yes.set_index(\u0026quot;job\u0026quot;)  Subscribed Job Total Prevalence Rate job student 275 875 0.314286 retired 434 1720 0.252326 unemployed 144 1014 0.142012 admin. 1352 10422 0.129726 management 328 2924 0.112175 unknown 37 330 0.112121 technician 730 6743 0.108260 self-employed 149 1421 0.104856 housemaid 106 1060 0.100000 entrepreneur 124 1456 0.085165 services 323 3969 0.081381 blue-collar 638 9254 0.068943 Perhaps surprisingly, students have the highest prevalence (~31.4%) albeit being one of the least targeted demographics. By contrast blue-collar prospects were second most called (see “Education Levels” section) yet had the lowest conversion rate at ~ 6.9%. This plot confirms raw values give an incomplete picture of conversion. We can summarize the first fact with this simple text graphic below.\nfig, ax = plt.subplots(figsize=(6,6)) _=plt.text(0.29, 0.6, \u0026quot;31%\u0026quot;, size=40, va=\u0026quot;baseline\u0026quot;, ha=\u0026quot;right\u0026quot;, multialignment=\u0026quot;left\u0026quot;, color = \u0026quot;green\u0026quot; ) _=plt.text(0.8, 0.6, \u0026quot;of students converted...\u0026quot;, size=16, va=\u0026quot;baseline\u0026quot;, ha=\u0026quot;right\u0026quot;, multialignment=\u0026quot;left\u0026quot; ) _=plt.text(0.15, 0.4, \u0026quot;...yet made up only\u0026quot;, size=16, va=\u0026quot;baseline\u0026quot;, ha=\u0026quot;left\u0026quot;, multialignment=\u0026quot;left\u0026quot; ) _=plt.text(0.57, 0.4, \u0026quot;2%\u0026quot;, size=20, va=\u0026quot;baseline\u0026quot;, ha=\u0026quot;left\u0026quot;, multialignment=\u0026quot;left\u0026quot;, color=\u0026quot;red\u0026quot; ) _=plt.text(0.67, 0.4, \u0026quot;of contacts\u0026quot;, size=16, va=\u0026quot;baseline\u0026quot;, ha=\u0026quot;left\u0026quot;, multialignment=\u0026quot;left\u0026quot; ) plt.gca().axes.get_yaxis().set_visible(False) plt.gca().axes.get_xaxis().set_visible(False) plt.show() The bank should consider reaching out to students more often.\n Prevalence Rates by Education Level We now do the same analysis for education.\neduc_yes = (bankingDF[bankingDF[\u0026quot;y\u0026quot;] == \u0026quot;yes\u0026quot;] .groupby(\u0026#39;education\u0026#39;) .agg({\u0026#39;y\u0026#39;:\u0026#39;count\u0026#39;}) .rename(columns = {\u0026#39;y\u0026#39;:\u0026#39;Subscribed\u0026#39;}) .reset_index()) # total number of people in each job category # regardless of subscription status all_educ = (bankingDF .groupby(\u0026#39;education\u0026#39;) .agg({\u0026#39;y\u0026#39;:\u0026#39;count\u0026#39;}) .rename(columns = {\u0026#39;y\u0026#39;:\u0026#39;Education Total\u0026#39;}) .reset_index()) # calculate prevalence rates educ_yes[\u0026#39;Education Total\u0026#39;] = all_educ[\u0026quot;Education Total\u0026quot;] educ_yes[\u0026quot;Prevalence Rate\u0026quot;] = educ_yes[\u0026quot;Subscribed\u0026quot;]/educ_yes[\u0026quot;Education Total\u0026quot;] educ_yes = educ_yes.sort_values(\u0026quot;Prevalence Rate\u0026quot;, ascending=False) _=sns.barplot(educ_yes[\u0026quot;education\u0026quot;], educ_yes[\u0026quot;Prevalence Rate\u0026quot;], color=\u0026quot;skyblue\u0026quot;) _=plt.tick_params(axis=\u0026quot;x\u0026quot;, labelsize=8, rotation=30) _=plt.tick_params(axis=\u0026quot;y\u0026quot;, labelsize = 11) _=plt.ylabel(\u0026quot;Prevalence Rate\u0026quot;, fontsize = 12) plt.show() educ_yes.set_index(\u0026quot;education\u0026quot;)  Subscribed Education Total Prevalence Rate education illiterate 4 18 0.222222 unknown 251 1731 0.145003 university.degree 1670 12168 0.137245 professional.course 595 5243 0.113485 high.school 1031 9515 0.108355 basic.4y 428 4176 0.102490 basic.6y 188 2292 0.082024 basic.9y 473 6045 0.078246 With only 18 illiterate contacts it’s no surprise seeing an inflated prevalence rate for that group. Just like job category, we see an asymmetry in education level: those with an unknown education fall nearly last in terms of being contacted (see “Education level of prospects” section) but converted second most often (or the most often if you ignore contacts who were illiterate).\n  Extra features Here are some other variables that could widen the scope of analysis:\n (Monthly) Income: this adds more info than loan or default Current bank provider: if someone has a different bank than this Portugal bank, they may be less likely to accept Does the prospect have kids? If so, what is the current education level or age of the kid(s)? Is the bank agent who called male or female? Is the prospect male or female?  A (major) caveat is some of this info may be thought too personal to share.\n  Extras Rejecting the bank: what are the odds? For individuals with similar age, education, job, and marital status, what are the odds someone with a personal loan rejecting the bank’s offer compared to that of someone without a personal loan rejecting the bank’s offer? We’ll look at single college graduates in the age range 30-40 who work in administrative positions. These characteristics comprise one of the largest demographics targed by the bank.\n# getting the desired sample (retrospectively) bank_retro = bankingDF[(bankingDF[\u0026quot;age\u0026quot;].between(30,40)) \u0026amp; (bankingDF[\u0026quot;education\u0026quot;] == \u0026quot;university.degree\u0026quot;) \u0026amp; (bankingDF[\u0026quot;job\u0026quot;]==\u0026quot;admin.\u0026quot;) \u0026amp; (bankingDF[\u0026quot;marital\u0026quot;] == \u0026quot;single\u0026quot;) \u0026amp; (bankingDF[\u0026quot;loan\u0026quot;] != \u0026quot;unknown\u0026quot;)] # two way table summary retro_tab = pd.crosstab(bank_retro[\u0026quot;loan\u0026quot;], bank_retro[\u0026quot;y\u0026quot;], margins=True, margins_name = \u0026quot;Total\u0026quot;) retro_tab.columns.names = [\u0026quot;Subscribed?\u0026quot;] retro_tab.index.names = [\u0026quot;Personal Loan?\u0026quot;] retro_tab Subscribed? no yes Total Personal Loan? no 976 154 1130 yes 226 23 249 Total 1202 177 1379 From this we see, for instance, 226 people who rejected the bank (i.e. did not subscribe) had a personal loan. We can compute the odds ratio (the odds of rejecting) as follows:\n Odds of saying no, given that one has a personal loan: \\(\\frac{226}{23}\\) Odds of saying no, given that one doesn’t have a personal loan: \\(\\frac{976}{154}\\)  Therefore the odds of rejecting the bank’s offer if one has a personal loan is (226/23)/(976/154) = 1.55 times higher than the odds of rejecting the bank if one has no personal loan. This applies only to those with the prescribed characterisitcs above: between 30-40 years old, college grad, single, and working in admin.\nEquivalently, the odds of someone with a personal loan rejecting the bank is 55% higher than the odds of someone without a personal loan rejecting the bank. This (again) applies only to those with the prescribed characterisitcs just mentioned.\nIf the banking agency chose its contacts based off of random sampling, then these inferences are applicable to those in the general public in the same demographic. Said differently, the scope of inference wouldn’t be restricted to just those in the current dataset. We’ll assume for simplicity that random sampling was used.\nThe upshot would then be this: the bank runs a higher risk of rejection if people in the aforementioned demographics have personal loans, which is pretty intuitive - people in debt are less likely to fork out money.\n  ",
    "ref": "/project/conversion-portuguese-bank/"
  },{
    "title": "Working around LaTeX and Plotly issue in Hugo Minimal",
    "date": "",
    "description": "",
    "body": "       I’ve noticed some wierd quirks about Hugo Minimal, which is the theme used for this website. There’s no indication it supports by default rendering of LaTeX with MathJax. However, I have no problem rendering LaTeX - see this post of mine for example.\nWhat is problematic for me is including a plotly chart in a document that contains LaTeX. Plotly is nice if you want to include interactive plots in your documents. Here’s a scatterplot from the mtcars dataset.\nlibrary(plotly) plot_ly(mtcars, x = ~hp, y = ~mpg)  {\"x\":{\"visdat\":{\"4ea21e35758d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"4ea21e35758d\",\"attrs\":{\"4ea21e35758d\":{\"x\":{},\"y\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"hp\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"mpg\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],\"y\":[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} For some reason if I include a plotly chart, LaTeX typed in the document will not render.\n\\(c^2 = a^2 + b^2 - 2abcos(C)\\)\nAs shown above, you instead see the raw LaTeX.\nThe workaround that checks-out for me is using a LaTeX online editor like CodeCogs. The website converts math expressions you type to LaTeX. They automatically produce html code that can be copied and pasted into your document – even inline! So, I can write the law of cosines formula is  with no issue, thanks to the html produced by CodeCogs. It looks like this:\n\u0026lt;a href=\u0026quot;https://www.codecogs.com/eqnedit.php?latex=\\inline\u0026amp;space;c^2\u0026amp;space;=\u0026amp;space;a^2\u0026amp;space;\u0026amp;plus;\u0026amp;space;b^2\u0026amp;space;-\u0026amp;space;2abcos(C)\u0026quot; target=\u0026quot;_blank\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;https://latex.codecogs.com/gif.latex?\\inline\u0026amp;space;c^2\u0026amp;space;=\u0026amp;space;a^2\u0026amp;space;\u0026amp;plus;\u0026amp;space;b^2\u0026amp;space;-\u0026amp;space;2abcos(C)\u0026quot; title=\u0026quot;c^2 = a^2 + b^2 - 2abcos(C)\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; So I don’t know why LaTeX code renders natively without modifying Minimal’s Javascript, or why it it doesn’t render when I include plotly charts. One of those is a happy coincidence. Thankfully for the other one, you can turn to CodeCogs to sort it out!\n",
    "ref": "/post/latex-plotly-issue-hugo-minimal/"
  },{
    "title": "Proving the Properties of the Mean and Variance of Standardized Data",
    "date": "",
    "description": "",
    "body": " Some Background There’s a lot of concepts we take for granted in applied statistics, like the CLT or properties of the mean and variance for standardized data. And that’s totally fine! Our job as applied analysts is to properly understand and implement statistical methods to derive value for whomever our stakeholders are.\nThat being said, it’s a good idea whenever possible to understand why things are the way they are. For example, we’re familiar with transforming data by standaridizing (subtracting the mean of the data from each observation and dividing that difference by the standard deviation – shown in the below formula).\n\\[\\frac{X - \\mu}{\\sigma };X = {x_1, x_2, ..., x_n}\\]\nIt’s often the case we don’t know the true mean \\(\\mu\\) and true population variance \\({\\sigma}^2\\), so we use the sample mean \\(\\overline{x}\\) and sample variance \\(s^2\\). So the above formula translates to\n\\[\\frac{X - \\overline{x}}{s};X = {x_1, x_2, ..., x_n}\\]\nTherefore, the standardized data looks like this\n\\[\\left \\{ \\frac{x_1 - \\overline{x}}{s}, \\frac{x_2 - \\overline{x}}{s}, ... , \\frac{x_n - \\overline{x}}{s} \\right \\}\\]\nThis transformed data has a mean of 0 and standard deviation of 1 — and hence a variance of 1. I worked this out by hand to see why that’s the case. To do that, we need to reference the formulas for calculating the sample mean and sample variance of a dataset.\nSample Mean Formula \\[\\overline{x} = \\sum_{i=1}^{n}x_i = \\frac{1}{n}(x_1 + x_2 + ... + x_n)\\]\n Sample Variance Formula \\[s^2 = \\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^{2}}{n-1} = \\frac{1}{n-1}[(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + ... + (x_n - \\overline{x})^2]\\]\n  The mean of the standardized data We’re finding the mean of \\(\\left \\{ \\frac{x_1 - \\overline{x}}{s}, \\frac{x_2 - \\overline{x}}{s}, ... , \\frac{x_n - \\overline{x}}{s} \\right \\}\\), so this is\n\\(\\frac{1}{n}\\left ( \\frac{x_1 - \\overline{x}}{s} + \\frac{x_2 - \\overline{x}}{s}+ ... + \\frac{x_n - \\overline{x}}{s} \\right )\\)\nFactor out \\(1/s\\)\n\\(\\frac{1}{ns}\\left [(x_1 - \\overline{x})+ (x_2 - \\overline{x})+ ... +(x_n - \\overline{x}) \\right]\\)\nThere are \\(n\\) terms of \\(\\overline{x}\\), so we now have\n\\(\\frac{1}{ns}\\left [(x_1 + x_2 + ... +x_n) - nx \\right]\\)\nFrom the definition of the sample mean, we know the terms in the parenthesis is equivalent to \\(n\\overline{x}\\) (by multiplying both sides of the sample mean equation by \\(n\\)), so we get\n\\(\\frac{1}{ns}\\left (n\\overline{x} - n\\overline{x} \\right) = \\frac{1}{ns}*0 = 0\\)\nWe’ve just shown the mean of the standardized data is equal to 0.\n The variance of the standardized data Drink Yoda GIF from Drink GIFs   Take a break if you need one, because finding the variance is a bit more tricky!\nWe want\n\\(Var\\left \\{ \\frac{x_1 - \\overline{x}}{s}, \\frac{x_2 - \\overline{x}}{s}, ... , \\frac{x_n - \\overline{x}}{s} \\right \\}\\)\nThis translates to\n\\(\\frac{1}{n-1}\\left [(\\frac{x_1 - \\overline{x}}{s} - \\overline{x}^*)^2 + (\\frac{x_2 - \\overline{x}}{s} - \\overline{x}^*)^2 + ... +(\\frac{x_n - \\overline{x}}{s} - \\overline{x}^*)^2 \\right ]\\)\nwhere \\(\\overline{x}^*\\) is the mean of the standardized data, NOT the original sample data! Note that we now have two sample means. One of the sample means (\\(\\overline{x}\\)) is the mean of the original dataset used for standardizing each observation in that dataset. The second mean (\\(\\overline{x}^*\\)) is the mean of the standardized data, and we use this to calculate the variance of the standardized data.\nWe just found the mean of the standardized data… it’s 0! So now the variance of the standardized data reduces to\n\\(\\frac{1}{n-1}\\left [(\\frac{x_1 - \\overline{x}}{s})^2 + (\\frac{x_2 - \\overline{x}}{s})^2 + ... +(\\frac{x_n - \\overline{x}}{s})^2 \\right ]\\)\nLet’s factor out the \\(s^2\\) from each term.\n\\(\\frac{1}{(n-1)s^2}\\left [(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + ... +(x_n - \\overline{x})^2 \\right ]\\)\nHow do we deal with the terms in the square brackets? Look back at the sample variance formula – the terms inside are the same! Multiply both sides by \\(n-1\\): the terms in the bracket are equivalent to \\((n-1)s^2\\)\nSo we’re left with\n\\(\\frac{1}{(n-1)s^2}\\left [(n-1)s^2\\right] = 1\\)\nWe’ve shown the variance of the standardized data is indeed 1.\n Wrapping up Standardization is often used in machine learning applications like linear regression and KNN. I hope this post sheds some insight into how standardization works!\n ",
    "ref": "/post/mean-variance-standardized-data/"
  },{
    "title": "Forecasting Capital BikeShare Ridership",
    "date": "",
    "description": "",
    "body": "     Capital BikeShare docking station  Introduction I love to bike, especially during the spring and summer. As a New York City resident the city affords miles of urban bike paths, my favorite being the seamless connection between Riverside and Hudson River parks. Of course thousands (if not millions) of others enjoy biking throughout the country, and if you don’t own a bike you can rent one through various bike programs, like NYC’s Citi Bike and Washington DC’s Capital BikeShare (CB henceforth). The latter is the focus of this report.\n Background and Motivation CB has maintained ridership data since 2010, but I first came across their data on the UCI Machine Learning Repository here which featured data from 2011-2012. A more complete analysis of CB’s ridership requires more data, so UCI’s data will be merged with 2013-2017 data from CB’s website.\nThe objective is to create 30-day trip forecasts by comparing several competing time series models. This task is relevant to CB officials since they could predict how much revenue to expect for any 30 day period. While location-based data isn’t utilized in this analysis, using it could help identify areas needing increased bike availability. And although daily data is used here, hourly forecasts would enable cyclists to gauge and anticipate future bike demand.\n Literature Review Bike share programs are relatively new, but research on factors affecting ridership has been conducted. Buck and Buehler 2011 have conducted research on CB ridership at the station level during CB’s first six months of operation. Using stepwise multiple regression model, they conclude average daily ridership is affected by the number of bike lines within 1/2 mile of a station, the number of Washington DC Alcohol Beverage Regulation Administration (ABRA) license holders, and the weighted average percentage of households with no access to an automobile (adjusted \\(R^2\\) = 0.66, n=91).\nRixey 2012 investigates the natural logarithm of monthly bike rentals across three bike share programs (including CB) from the first operating season of each, on a station basis using multiple regression. Rixey reduces his 19 candidate variables to 11 (plus an intercept) that include various socioeconomic and bike system architecture variables, yielding an adjusted \\(R^2\\) of ~0.8.\nEl-Assi, et al 2017 perform a similar types of analysis of Toronto’s bike share program, developing models for weekday, weekend, station origin, and station destination ridership. Adjusted \\({R^2}\\) values hover around 0.66.\nWang et al 2012 use 19 total socioeconomic variables to develop a regression model for station level, 2011 ridership of the Nice Ride bike share program in Minneapolis. Notably no weather variables are included. They achieve an adjusted Adjusted \\({R^2}\\) of approximately 0.87.\nAll of that research focuses more on identifying important variables and trends for ridership but not forecasting (extrapolating) future ridership. That is the focus of this analysis. It’s important to note high \\(R^2\\) values don’t necessarily translate into accurate forecasts since models can overfit the data. That issue was not examined in their analyses, so it’s unsure how predictive their models are. However, the variables they used may be helpful additions to this analysis (at some later date).\n Data Cleaning and Feature Engineering Data comes from three sources: UCI 2011-2012 data, historical weather data from NOAA for 2011-2017, and the CB ridership data from 2013-2017. We’ll go through each one; as the code is sufficiently commented, summaries of the data wrangling should suffice.\nUCI Data (2011-2012) We begin with the UCI data since it’s the quickest and serves as the baseline for how the remainder of data from 2013-2017 is handled.\nRelevant revisions include:\n removing unnecessary weather and calendar variables - they’ll be replaced later on with other variables converting relevant calendar variables into factor/categorical variables converting weather data to their original, non-normalized scales adding a week of the month column. This isn’t completely necessary now, but it’s used later to define/encode holiday dates in the 2013-2017 data  # main libraries used; others will be loaded later library(tswge) library(fpp2) library(lubridate) library(dplyr) ############## Begin with UCI data ############## # import 2011 and 2012 UCI data - will be merged with 2013-2017 data later # change file path to match yours! bikes_uci \u0026lt;- read.csv(\u0026quot;day.csv\u0026quot;) # remove atemp since it adds no info on top of temp # remove season and yr - unnecessary calendar variables # remove instant; remove workingday - weekday covers this # remove weathersit - cumbersome to define; this is replaced with actual rain and snowfall data bikes_uci \u0026lt;- bikes_uci[, -c(1,3,4,8,9,11)] # rename dteday to TripDate colnames(bikes_uci)[1] \u0026lt;- \u0026quot;TripDate\u0026quot; # add 1 to weekday to make consistent with everyday understanding (1-7) bikes_uci$weekday \u0026lt;- bikes_uci$weekday+1 # convert calendar variables to factor variables bikes_uci[, 2:4] \u0026lt;- lapply(bikes_uci[, 2:4], factor) # convert date column to ymd format bikes_uci$TripDate \u0026lt;- ymd(bikes_uci$TripDate) # add weekofmon (needed later for the other years\u0026#39; holiday calculations) bikes_uci \u0026lt;- bikes_uci %\u0026gt;% mutate(weekofmon = ifelse(between(day(TripDate),1,7), 1, ifelse(between(day(TripDate),8,14), 2, ifelse(between(day(TripDate),15,21), 3, 4)))) # un-normalize temp; normalization formula: (t-t_min)/(t_max-t_min); t_max = 39, t_min=-8 bikes_uci$temp \u0026lt;- 47*bikes_uci$temp - 8 # convert temp from celcius to farenheit bikes_uci$temp \u0026lt;- bikes_uci$temp * 9/5 + 32 # un-normalize humidity; normalization formula: hum/100 bikes_uci$hum \u0026lt;- 100*bikes_uci$hum # convert weekofmon to factor bikes_uci$weekofmon \u0026lt;- as.factor(bikes_uci$weekofmon)  Capital BikeShare Data (2013-2017) The five years of data amount to nearly 16 million rows combined. Since UCI’s data is aggregated on a daily scale, we do the same here and include variables found in the UCI dataset. The time-consuming task here is encoding whether or not a date is a federal holiday, since some holidays change dates year to year. ThoughtCo has a helpful page describing the “calendar rules” of federal holidays. By creating calendar-based columns like month, week of month, and day of the week, we can easily create a function that checks calendar conditions satisfying a federal holiday.\nlibrary(data.table) ############## Now we import and clean the 2013-2017 data ############## # data can be found from [here](https://www.capitalbikeshare.com/system-data) # read as data.table for faster import b \u0026lt;- list.files(full.names = TRUE)[1:20] %\u0026gt;% lapply(fread) %\u0026gt;% bind_rows # convert to a dataframe to use dplyr functions b \u0026lt;- data.frame(b) # convert Start date to ymd:hms format b$Start.date \u0026lt;- ymd_hms(b$Start.date, tz=\u0026quot;US/Eastern\u0026quot;) # make a new column just with the date only b$TripDate \u0026lt;- as.Date(b$Start.date, tz=\u0026quot;US/Eastern\u0026quot;) # compute number of cyclists by date cyclists \u0026lt;- b %\u0026gt;% group_by(TripDate) %\u0026gt;% summarise(casual = sum(Member.type == \u0026quot;Casual\u0026quot;), registered = sum(Member.type == \u0026quot;Member\u0026quot;), cnt = casual+registered) %\u0026gt;% mutate(weekday = wday(TripDate), mnth = month(TripDate), weekofmon = ifelse(between(day(TripDate),1,7), 1, ifelse(between(day(TripDate),8,14), 2, ifelse(between(day(TripDate),15,21), 3, 4)))) ## - Let\u0026#39;s define federal holidays # fixed holidays holidays \u0026lt;- c(ymd(\u0026quot;2013/01/01\u0026quot;) + years(0:4), # New Years Day ymd(\u0026quot;2013/12/25\u0026quot;) + years(0:4), # Christmas Day ymd(\u0026quot;2013/11/11\u0026quot;) + years(0:4), # Veterans Day ymd(\u0026quot;2013/07/04\u0026quot;) + years(0:4)) # Independence Day # holidays that change dates; weekday ranges from 1-7, with 1 being Sunday # info taken from https://www.thoughtco.com/public-holidays-in-the-united-states-3368327 is_moving_hday \u0026lt;- function() { # Thanksgiving - fourth Thursday of November (cyclists$mnth == 11 \u0026amp; cyclists$weekofmon == 4 \u0026amp; cyclists$weekday == 5) | # MLK\u0026#39;s Birthday - third Monday of January (cyclists$mnth == 1 \u0026amp; cyclists$weekofmon == 3 \u0026amp; cyclists$weekday == 2) | # George Washington\u0026#39;s Birthday - third Monday of February (cyclists$mnth == 2 \u0026amp; cyclists$weekofmon == 3 \u0026amp; cyclists$weekday == 2) | # Memorial Day - fourth Monday of May (cyclists$mnth == 5 \u0026amp; cyclists$weekofmon == 4 \u0026amp; cyclists$weekday == 2) | # Labor Day - first Monday of September (cyclists$mnth == 9 \u0026amp; cyclists$weekofmon == 1 \u0026amp; cyclists$weekday == 2) | # Columbus Day - second Monday of October (cyclists$mnth == 10 \u0026amp; cyclists$weekofmon == 2 \u0026amp; cyclists$weekday == 2) } # create a holiday column in cyclists data frame cyclists \u0026lt;- cyclists %\u0026gt;% mutate(holiday = ifelse((is_moving_hday() | TripDate %in% holidays), 1, 0)) # convert calendar variables to factor variables cyclists[, 5:8] \u0026lt;- lapply(cyclists[, 5:8], factor)  NOAA Historical Weather Data (2011-2017) CB doesn’t include weather related variables like UCI does (the data contributor(s) added them separately), so we need to find this ourselves. It’s a fair assumption that temperature and precipitation affect ridership patterns so this is well worth finding. The data is included in my repository but here’s how you can get it yourself:\n1) go to https://www.ncdc.noaa.gov/cdo-web/datasets  2) under Local Climatological Data, select “Search Tool”  3) choose State, then Virginia  4) navigate to Washington Reagan National Airport (closest weather station to DC)  5) click Add to Cart  6) go to your cart in the upper right hand side of the webpage; click View All Items  7) choose LCD CSV, select dates 2011-01-01 to 2017-12-31, then click Continue  8) fill in your email to get the data, then click Submit Order  The data will take a couple of minutes to be sent to your email We’re only concerned with the date, temperature, precipitation, snow, wind speed, and humidity. Everything else can be discarded.\nAfter examining the data we see December 31 info is missing for 2011, 2013 and 2014 so these are searched manually from timeanddate, averaging the four six-hour interval values for the aforementioned weather variables (data is given for 12am, 6am, 12pm, and 6pm). Precipitation and snow values denoted as T (trace amounts) are replaced with 0.\n####### ---------- Include (dry-bulb) temperature, humidity, and precipitation data -------------### # change file path to match yours dc_weather \u0026lt;- read.csv(\u0026quot;dcweather.csv\u0026quot;) # non-blank temps represent daily averages dc_weather \u0026lt;- filter(dc_weather, !is.na(DailyAverageDryBulbTemperature)) # keep date, temp, and humidity columns - find their column numbers first cols_keep \u0026lt;- match(c(\u0026quot;DATE\u0026quot;,\u0026quot;DailyAverageDryBulbTemperature\u0026quot;,\u0026quot;DailyAverageRelativeHumidity\u0026quot;, \u0026quot;DailyAverageWindSpeed\u0026quot;, \u0026quot;DailyPrecipitation\u0026quot;, \u0026quot;DailySnowfall\u0026quot;), colnames(dc_weather)) # remove everything else dc_weather \u0026lt;- dc_weather[, cols_keep] # shorten/rename columns colnames(dc_weather) \u0026lt;- c(\u0026quot;TripDate\u0026quot;, \u0026quot;temp\u0026quot;, \u0026quot;hum\u0026quot;, \u0026quot;windspeed\u0026quot;, \u0026quot;prec\u0026quot;, \u0026quot;snow\u0026quot;) # remove \u0026quot;T\u0026quot; and time values from date column; first convert to character # equivalent to keeping elements 1-10 of the date string dc_weather$TripDate \u0026lt;- as.character(dc_weather$TripDate) dc_weather$TripDate \u0026lt;- substring(dc_weather$TripDate,1,10) # now convert TripDate column to TripDate object dc_weather$TripDate \u0026lt;- ymd(dc_weather$TripDate) # quick check of weather data (using summary function) reveals # T values in prec and snow; T means trace amounts, so we\u0026#39;ll substitute 0 for T # info taken from https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf # first need to convert from factor to character - can\u0026#39;t directly substitute from factor dc_weather[, 5:6] \u0026lt;- apply(dc_weather[, 5:6], 2, function(x) as.character(x)) # now make the substitution for (i in 5:6) { # if any value in column i equals T, replace with 0, otherwise keep original value dc_weather[, i] \u0026lt;- ifelse(dc_weather[, i] == \u0026quot;T\u0026quot;, \u0026quot;0\u0026quot;, dc_weather[, i]) } # change prec and snow columns to numeric type dc_weather[, 5:6] \u0026lt;- apply(dc_weather[, 5:6], 2, function(x) as.numeric(x)) # missing 12/31 date for 2011, 2013, and 2014. insert rows into dc_weather # find row number Dec 30 for \u0026#39;11, \u0026#39;13, and \u0026#39;14. dec_30s \u0026lt;- which(month(dc_weather$TripDate) == 12 \u0026amp; day(dc_weather$TripDate) == 30 \u0026amp; year(dc_weather$TripDate) %in% c(2011,2013,2014)) # create a small dataframe to hold Dec 31 info dec_31_info \u0026lt;- data.frame(TripDate = ymd(c(\u0026quot;2011/12/31\u0026quot;, \u0026quot;2013/12/31\u0026quot;, \u0026quot;2014/12/31\u0026quot;)), temp = c(58,41,36), hum = c(62,49,43), windspeed = c(9,11,7), prec = c(0,0,0), snow = c(0,0,0) ) # cut and paste together all the weather data dc_weather \u0026lt;- rbind(dc_weather[1:364, ], # 1/1/2011 - 12/30/2011 dec_31_info[1,], # 12/31/2011 dc_weather[365:1094, ], # 1/1/2012 - 12/30/2013 dec_31_info[2, ], # 12/31/2013 dc_weather[1095:1458, ], # 1/1/2014 - 12/30/14 dec_31_info[3,], # 12/31/14 dc_weather[1459:2919, ] # everything else )   Combining all three datasets Finally, we unite all three datasets to obtain 2011-2017 ridership data. First, we replace the deleted UCI weather data with the 2011-2012 info from NOAA. Then we update the CB data by attaching the NOAA weather data from 2013-2017 to it. Finally, we horizontally stack the UCI and CB data (2011-2012 on top, 2013-2017 on the bottom) to make one complete dataset called bikes.\n############## combining all three datasets ############## # add prec and snow values for 2011 and 2012 to bikes_uci # these are the substitutes for weathersit bikes_uci \u0026lt;- dc_weather %\u0026gt;% filter(between(TripDate, \u0026quot;2011/01/01\u0026quot;, \u0026quot;2012/12/31\u0026quot;)) %\u0026gt;% select(TripDate, prec, snow) %\u0026gt;% merge(bikes_uci, by=\u0026quot;TripDate\u0026quot;, all=T) # merge dc_weather and cyclists -- this adds on the weather data to cyclists bikes_temp \u0026lt;- merge(cyclists, dc_weather, by=\u0026quot;TripDate\u0026quot;) # now we have all 2013-2017 data. need to combine this with UCI 2011-2012 data # first need to reorder columns of bikes_uci (or bike_temp) cols_matched \u0026lt;- match(colnames(bikes_temp), colnames(bikes_uci)) bikes_uci \u0026lt;- bikes_uci[, cols_matched] # now merge horizontally (stack UCI data on top of 2013-2017) bikes \u0026lt;- bind_rows(bikes_uci, bikes_temp)  Final Data Cleaning Curiously, all humidity data from October 2013 is missing (as well as for four other dates).\nbikes[which(is.na(bikes$hum)), c(1,4,9:11)]  TripDate cnt temp hum windspeed 1005 2013-10-01 10255 72 NA 5.0 1006 2013-10-02 10249 76 NA 5.8 1007 2013-10-03 10059 76 NA 4.3 1008 2013-10-04 10158 78 NA 3.2 1009 2013-10-05 10456 79 NA 4.6 1010 2013-10-06 9650 80 NA 8.8 1011 2013-10-07 5797 68 NA 12.3 1012 2013-10-08 9829 62 NA 8.2 1013 2013-10-09 6360 60 NA 12.7 1014 2013-10-10 2491 58 NA 13.4 1015 2013-10-11 1775 62 NA 11.0 1016 2013-10-12 5191 66 NA 11.2 1017 2013-10-13 5938 63 NA 10.8 1018 2013-10-14 8818 64 NA 6.7 1019 2013-10-15 9272 64 NA 3.6 1020 2013-10-16 9629 68 NA 4.9 1021 2013-10-17 9468 70 NA 7.7 1022 2013-10-18 10256 61 NA 6.6 1023 2013-10-19 9309 60 NA 6.9 1024 2013-10-20 8493 58 NA 8.7 1025 2013-10-21 8956 57 NA 7.1 1026 2013-10-22 8990 60 NA 7.3 1027 2013-10-23 7884 54 NA 8.0 1028 2013-10-24 7823 48 NA 9.0 1029 2013-10-25 7866 48 NA 8.9 1030 2013-10-26 7906 46 NA 10.1 1031 2013-10-27 7917 53 NA 5.5 1032 2013-10-28 8515 55 NA 3.3 1033 2013-10-29 8949 55 NA 2.7 1034 2013-10-30 8055 59 NA 3.6 1035 2013-10-31 8335 62 NA 8.1 1520 2015-02-28 3222 27 NA 5.1 2013 2016-07-09 12198 86 NA 8.4 2157 2016-11-30 6170 61 NA 6.0 2247 2017-02-28 7828 58 NA 4.9 Fortunately we can use past values to impute missing data. The ACF plot below quantifies how strong current and past humidity values are correlated, such as ~ 0.5 correlation between a given day’s value and yesterday’s value.\nAcf(bikes$hum, lag.max = 30, main=\u0026quot;\u0026quot;) rect(xleft=0.5, ybottom=0, xright=1.5, ytop=0.53, border=\u0026quot;red\u0026quot;,lwd=2) arrows(x0=4, y0=0.45, x1=2, y1=0.45, angle=20, col=\u0026quot;red\u0026quot;, length=.1) text(x=3.7, y=0.45, labels = c(\u0026quot;Lag 1\u0026quot;), pos=4, cex=.85) title(\u0026quot;Humidity Autocorrelations\u0026quot;, line=1) Using one day previous values of humidity would be great if we weren’t missing 31 consecutive days! However, if we assign a seasonality of 365 to humidity, decomposing the humidity data displays a strong annual pattern, as seen in the seasonal component below.\n# remove NAs from humidity column hum_2 \u0026lt;- na.omit(bikes$hum) # make into ts object hum_2_ts \u0026lt;- ts(hum_2, frequency = 365) stl(hum_2_ts, s.window = \u0026quot;periodic\u0026quot;) %\u0026gt;% autoplot() + xlab(\u0026quot;Time (Years)\u0026quot;) + theme_minimal() So we resort to using last year’s date ( ~ 365 days) for each missing date since there’s evidence of annual seasonality. For example, we’ll use humidity from 10/12/2012 as a substitute for 10/12/2013. It’s quite natural to ask what last year’s value of something was if you’re missing this year’s data and have evidence of annual seasonality, as we do here.\n############## final data cleaning ############## # find date of missing hum values and get the previous year\u0026#39;s date last_yrs \u0026lt;- bikes[which(is.na(bikes$hum)), ]$TripDate - years(1) # get the associated humidity values last_hum \u0026lt;- bikes %\u0026gt;% filter(TripDate %in% last_yrs) %\u0026gt;% select(TripDate, hum) # impute by first storing relevant row numbers rows_to_impute \u0026lt;- which(is.na(bikes$hum)) bikes[rows_to_impute, 10] \u0026lt;- last_hum$hum # to model trend (in MLR model), need to include time variable (Time) # Time is simply the length of the time series (1:nrow(bikes)) bikes$Time \u0026lt;- 1:nrow(bikes)  Lastly, we create a variable named Time which ranges from 1 to the total number of rows in bikes 2553, which implies Day 1 through 2553 of the dataset. Later on we’ll use this to model trend in our regression models.\n Summary There’s a lot in this section! It reflects how many data science analyses go: 70-80% of time is spent data cleaning and feature engineering, and there’s even more of the latter later in the modeling stage as we improve model performance. For now let’s restate what we’ve done so far.\n Replace categorical weather info in the UCI dataset with numerical values of snow and precipitation from NOAA Introduce week of month as an auxiliary feature, and use it alongside month of year to define federal holiday as a feature in the 2013-2017 data Impute missing weather data missing from NOAA Introduce a time variable to model trend (later in the analysis)  At this point we can (finally) explore the data!\n  Exploratory Analysis Data Dictionary Before exploring, here’s a summary of all the variables and their meanings.\nlibrary(kableExtra) bike_variables \u0026lt;- data.frame(Variable = c(\u0026quot;TripDate\u0026quot;, \u0026quot;casual\u0026quot;, \u0026quot;registered\u0026quot;, \u0026quot;cnt\u0026quot;, \u0026quot;weekday\u0026quot;, \u0026quot;mnth\u0026quot;, \u0026quot;weekofmon\u0026quot;, \u0026quot;holiday\u0026quot;, \u0026quot;temp\u0026quot;, \u0026quot;hum\u0026quot;, \u0026quot;windspeed\u0026quot;, \u0026quot;prec\u0026quot;, \u0026quot;snow\u0026quot;, \u0026quot;Time\u0026quot;), Description = c(\u0026quot;Date of record\u0026quot;, \u0026quot;Number of trips made by lower-tier membership plan riders\u0026quot;, \u0026quot;Number of trips made by upper-tier membership plan riders\u0026quot;, \u0026quot;Sum of the two previous columns\u0026quot;, \u0026quot;Day of the week. 1 = Sunday\u0026quot;, \u0026quot;Month of TripDate (1-12)\u0026quot;, \u0026quot;Week of month of the TripDate (1-4)\u0026quot;, \u0026quot;Is the TripDate a federal holiday? 1 = Yes\u0026quot;, \u0026quot;Temperature in Fahrenheit\u0026quot;, \u0026quot;Humidity expressed as a percent (maximum possible = 100)\u0026quot;,\u0026quot;Wind speed in miles per hour\u0026quot;, \u0026quot;Amount of precipitation (rain) in inches\u0026quot;,\u0026quot;Amount of snowfall in inches\u0026quot;, \u0026quot;Day Number (Day 1, Day 2, etc). Used to model trend in regression models\u0026quot;)) bike_variables %\u0026gt;% kable() %\u0026gt;% kable_styling(full_width = F, bootstrap_options = \u0026quot;striped\u0026quot;)   Variable  Description      TripDate  Date of record    casual  Number of trips made by lower-tier membership plan riders    registered  Number of trips made by upper-tier membership plan riders    cnt  Sum of the two previous columns    weekday  Day of the week. 1 = Sunday    mnth  Month of TripDate (1-12)    weekofmon  Week of month of the TripDate (1-4)    holiday  Is the TripDate a federal holiday? 1 = Yes    temp  Temperature in Fahrenheit    hum  Humidity expressed as a percent (maximum possible = 100)    windspeed  Wind speed in miles per hour    prec  Amount of precipitation (rain) in inches    snow  Amount of snowfall in inches    Time  Day Number (Day 1, Day 2, etc). Used to model trend in regression models     Our variable of interest to predict is cnt.\n Overall Daily Ridership The line plot below is interactive, so feel free to click and drag around your mouse to zoom in on any portion. We see a clear positive trend and annual seasonality in ridership – for example, people tend to ride more during the summer than winter months.\nThe tall upward peaks in 2014 and 2015 most likely reflect high turnout at the annual National Cherry Blossom Festival.\nggplot(bikes, aes(TripDate,cnt)) + geom_line(col=\u0026quot;steelblue\u0026quot;) + xlab(\u0026quot;\u0026quot;)+ ylab(\u0026quot;Total Trips\u0026quot;)+ scale_x_date(breaks = \u0026quot;11 months\u0026quot;) + ggtitle(\u0026quot;Capital BikeShare Daily Trips (2011-2017)\u0026quot;) + theme(panel.grid = element_blank(), plot.title = element_text(hjust=0.5, family = \u0026quot;Georgia\u0026quot;))  Exploring Weather Variables on Ridership Below is a scatterplot matrix of continuous predictors (weather variables) plotted against each other and the response variable cnt (total number of trips). Some comments:\n No significant multicollinearity exists between the weather variables. This means regression coefficients for those variables should be stable Only temperature is strongly correlated with trip numbers Snowfall and precipitation negatively affect trips (as expected)  pairs(bikes[, c(4,9:13)], lower.panel = NULL, col=\u0026quot;steelblue\u0026quot;)  Ridership by Day of the Week Trip patterns tend to fluctuate depending on day of week though not in a drastic way. Nevertheless people tend to ride more in the middle of the week (Wednesdays in particular, beginning in 2015) and less during the weekends.\nlibrary(numform) library(gganimate) library(magick) dow_animation \u0026lt;- bikes %\u0026gt;% group_by(weekday, Year = as.integer(year(TripDate))) %\u0026gt;% summarise(`Total Trips` = sum(cnt)) %\u0026gt;% ggplot(aes(weekday, `Total Trips`)) + geom_bar(stat = \u0026quot;identity\u0026quot;, fill=\u0026quot;lightblue\u0026quot;) + #geom_text(aes(label=paste0(round(`Total Trips`/1000), \u0026quot;K\u0026quot;)), vjust=1.6, size=3.5)+ scale_x_discrete(labels = c(\u0026quot;1\u0026quot;=\u0026quot;Sun\u0026quot;, \u0026quot;2\u0026quot;=\u0026quot;Mon\u0026quot;, \u0026quot;3\u0026quot;=\u0026quot;Tue\u0026quot;,\u0026quot;4\u0026quot;=\u0026quot;Wed\u0026quot;,\u0026quot;5\u0026quot;=\u0026quot;Thur\u0026quot;,\u0026quot;6\u0026quot;=\u0026quot;Fri\u0026quot;,\u0026quot;7\u0026quot;=\u0026quot;Sat\u0026quot;))+ scale_y_continuous(label=ff_thous(digits=2))+ theme(panel.background = element_blank(), axis.ticks = element_blank()) + labs(title=\u0026quot;Total Trips by Day of Week: {frame_time}\u0026quot;, x = \u0026quot;\u0026quot;, y=\u0026quot;Total Trips (in thousands)\u0026quot;)+ transition_time(Year) dow_anim \u0026lt;- animate(dow_animation, nframes = 50, renderer = magick_renderer()) # save as gif anim_save(\u0026quot;dayofweek.gif\u0026quot;, dow_anim)   Modeling \u0026amp; Forecasting Methodology To reiterate, we want to make 30 day ahead trip predictions. We’ll explore several models using (virtually) all of 2011-2016 as training data and an expanding window with a seven day skip period for 2017 as our test data. That was a mouthful. To rephrase, initial model parameters are estimated beginning with all 2011-2016 data. When it comes time to test, the training set is expanded to re-estimate model parameters. Here’s a tabular look at what the training and test sets look like.\nIt’s fixed origin since the training set always begins at 1/8/2011. We need to skip 1/1/2011 - 1/7/2011 since (spoiler alert) one of the later models uses a lag of 7 of cnt and we want to make model comparisons on the same test sets. We also skip seven days for subsequent training sets since it’d take much more time to run all possible models. For example, instead of making our second training set 1/8/2011 - 1/1/2017 (one day after 12/31/2016) we make it 1/8/2011 - 1/8/2017. So we sidestep having additional training sets 1/8/2011-1/1/2017, 1/8/2011-1/2/2017, … , 1/8/2011-1/7/2017. Test sets are always 30 days long. In total we have 42 training and test sets, created using the createTimeSlices function in the caret package.\nThe RMSE (root mean square error) and RMSLE (root mean square log error) is used to evaluate models. These metrics are different from each other in that RMSLE penalizes underestimates more stronger than overestimates, whereas RMSE sees both equally. However, both are used since some models have drastically lower RMSEs but almost identical RMSLEs. In a bike share context it’s logical to view underestimates more harshly than overestimates, that’s why RMSLE is considered in this analysis alongside RMSE.\nAfter the 42 RMSE and RMSLE values are calculated from a model we take the median of RMSE since a couple “bad” forecasts can quite skewer the true performance of our model, giving a false impression. We’ll use the mean of the RMSLEs since the RMSLE doesn’t suffer from that skewering.\nlibrary(caret) # use 2011-2016 for fitting model parameters; 2017 testing bikes_train \u0026lt;- bikes %\u0026gt;% filter(between(TripDate, \u0026quot;2011/01/08\u0026quot;, \u0026quot;2016/12/31\u0026quot;)) bikes_test \u0026lt;- bikes %\u0026gt;% filter(between(TripDate, \u0026quot;2017/01/01\u0026quot;, \u0026quot;2017/12/31\u0026quot;)) bikes_adj \u0026lt;- bikes[8:2553, ] # create training and test splits bike_splits \u0026lt;- createTimeSlices(bikes_adj$cnt, initialWindow = 2181, # row number of 12/31/2016 horizon = 30, fixedWindow = F, skip=7)  Model 1: Multiple Linear Regression (MLR) with autocorrelated residuals The pure time series ARIMA model doesn’t take advantage of the other variables in our dataset. A regression model can. Regression with time series data tends to create serially correlated residuals, violating a key MLR assumption. This is usually remedied by simultaneously computing the regression coefficients and residual structure. The seasonal period limitation of 350 in auto.arima (which performs the simultaneous calculations) doesn’t allow for this. As an alternative, we create two forecasts: 30 day forecasts from the “raw” MLR model and 30 days forecasts of the MLR’s residuals. Those are summed to create ridership forecasts.\nSelecting Predictors We don’t have too many variables, so let’s see which predictors produce the highest adjusted \\(R^{2}\\) and the lowest AIC. Using weekofmon as a predictor would be misleading since the fourth week has more than seven days most of the time. In the dataset nearly 200 extra days belong in the fourth week, so these extra days artificially inflate ridership numbers.\ntable(bikes$weekofmon)  1 2 3 4 588 588 588 789  library(olsrr) bike_reg \u0026lt;- lm(cnt ~ weekday+mnth+holiday+temp+hum+windspeed+prec+snow+Time, data=bikes_train) all_regs \u0026lt;- ols_step_all_possible(bike_reg) # using highest adj R^2 best_r2 \u0026lt;- all_regs %\u0026gt;% arrange(desc(adjr)) best_r2[1:5, c(2:3,5)] %\u0026gt;% kable(caption = \u0026quot;Predictors for best Adjusted R-Squared\u0026quot;,) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F) %\u0026gt;% row_spec(1, bold = T, color = \u0026quot;white\u0026quot;, background = \u0026quot;teal\u0026quot;)  Table 1: Predictors for best Adjusted R-Squared    n  predictors  adjr      9  weekday mnth holiday temp hum windspeed prec snow Time  0.8138955    8  weekday mnth holiday temp hum windspeed prec Time  0.8138678    8  weekday mnth holiday temp hum prec snow Time  0.8138655    7  weekday mnth holiday temp hum prec Time  0.8138267    7  weekday mnth temp hum prec snow Time  0.8103576     # using lowest AIC best_aic \u0026lt;- all_regs %\u0026gt;% arrange(aic) best_aic[1:5, c(2:3,8)] %\u0026gt;% kable(caption = \u0026quot;Predictors for best AIC\u0026quot;,) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F) %\u0026gt;% row_spec(1, bold = T, color = \u0026quot;white\u0026quot;, background = \u0026quot;orange\u0026quot;)  Table 1: Predictors for best AIC    n  predictors  aic      7  weekday mnth holiday temp hum prec Time  37976.46    8  weekday mnth holiday temp hum windspeed prec Time  37976.97    8  weekday mnth holiday temp hum prec snow Time  37976.99    9  weekday mnth holiday temp hum windspeed prec snow Time  37977.63    6  weekday mnth temp hum prec Time  38016.67     While adjusted \\(R^{2}\\) is a popular metric for regression, the AIC is usually used for forecasting. Since adj \\(R^{2}\\) for the best AIC model is virtually identical to that of the full model with all 9 variables, we’ll use the lowest AIC model. Those variables are highlighted in green; those in red are excluded.\n  Variables      weekday    mnth    weekofmon    holiday    temp    hum    windspeed    prec    snow    Time      Fitting and Forecasting Model 1 Fitting an MLR with the seven variables above produces residuals that evidence an s=364 seasonality and a need to perform first order differencing. The ACF residuals cuts off after lag 3 while the PACF (slowly) dampens to 0, which implies an MA(3) should be fit to the residuals. An MA(3) passes the Ljung-Box test (p-val of 0.176 with lag=30), strong evidence to conclude the MA(3) fit produces residuals resembling white noise. The ACF plot of the MA(3) gives visual confirmation. Since we took one seasonal and one nonseasonal difference and fit an MA(3) to the residuals, our final model is an MLR with an ARIMA(0,1,3)(0,1,0)[364] residual structure.\nbike_reg \u0026lt;- lm(cnt ~ weekday+mnth+holiday+temp+hum+prec+Time, data=bikes_train) reg_364 \u0026lt;- diff(bike_reg$residuals, lag=364) reg_364_1 \u0026lt;- diff(reg_364, 1) #Acf(reg_364_1, lag.max = 30) #Pacf(reg_364_1, lag.max = 30) reg_364_1_est \u0026lt;- est.arma.wge(reg_364_1, p=0, q=3, factor=F) Acf(reg_364_1_est$res, lag.max=30, main=\u0026quot;\u0026quot;) title(\u0026quot;ACF after fitting ARIMA(0,1,3)(0,1,0)[364] to Model 1\u0026#39;s Residuals\u0026quot;, line=1) #ljung.wge(reg_364_1_est$res, p=0, q=3, K=30) # 0.176 The forecasts yield a median RMSE of 2302.075 and a mean RMSLE of 0.269.\nrmses_mlr \u0026lt;- c() rmsle_mlr \u0026lt;- c() #42 is the number of train/test sets for (i in 1:42) { rows_train \u0026lt;- bike_splits$train[[i]] rows_test \u0026lt;- bike_splits$test[[i]] fit_reg \u0026lt;- lm(cnt ~ weekday+mnth+holiday+temp+hum+prec+Time, data=bikes_adj, subset = rows_train) # perform first order and seasonal differencing reg_364_1 \u0026lt;- diff(diff(fit_reg$residuals, 364), 1) # result is stationary, so now estimate parameters MA parameters reg_364_1_est \u0026lt;- est.arma.wge(reg_364_1, p=0, q=3, factor=F) # get forecast of residuals reg_res_for \u0026lt;- fore.aruma.wge(fit_reg$residuals, phi=reg_364_1_est$phi, theta=reg_364_1_est$theta, d=1, s=364, n.ahead = 30, limits=F, plot = F) # get forecast of raw MLR values reg_for \u0026lt;- predict(fit_reg, newdata = bikes_adj[rows_test, selected]) # add these forecasts together forecast_combined = reg_res_for$f + reg_for # calculate rmse err_rmse \u0026lt;- Metrics::rmse(bikes_adj$cnt[rows_test], forecast_combined) rmses_mlr \u0026lt;- append(rmses_mlr, err_rmse) # calculate rmsle err_rmsle \u0026lt;- Metrics::rmsle(bikes_adj$cnt[rows_test], forecast_combined) rmsle_mlr \u0026lt;- append(rmsle_mlr, err_rmsle) #rmse \u0026lt;- sqrt(mean(forecast_combined - bikes_adj$cnt[rows_test])^2) #rmses_mlr \u0026lt;- append(rmses_mlr, rmse) } #median(rmses_mlr) #mean(rmsle_mlr)   Model 2: Gradient Boosted Regression Trees All the models thus far considered only linear relationships. Tree-based methods like random forests and gradient boosted machines are non-parametric, exploiting non-linearities to creating custom if-then statements or rules for prediction (or classification) tasks. Gradient boosting trees are applied here to unlock further predictive accuracy.\nSince boosting uses tree-based methods, it does not automatically account for time series data. This is important since each tree in boosting is created by bootstrapping the training data in a random fashion. Order matters in time series, so random sampling can destruct time dependencies within data. Another issue arising from standard tree methods is failure to extrapolate increasing (or decreasing) trend for forecasting tasks. The final predictions from each tree during model building are averages of the response variable from the training set. The test set, by contrast, will include values of the response variable outside the range of the training set. This leads to systematically underpredicting test set data if there’s positive trend, or systematically overpredicting if there’s negative trend. Check this Medium post for a visual example.\nOur data has clear positive trend (see Overall Daily Ridership) section). To remedy the issues of extrapolation, the response variable is detrended using first-order differencing on the cnt variable (which is called dif1). The model and it’s predictions are based on the dif1, but the final predictions are back transformed to the original scale using the following formula:\n\\(PredictedOriginal_{t+1} = PredictedDiff_{t+1} + Actual_t\\)\nIn other words, tomorrow’s predicted ridership (on the original scale) is the sum of tomorrow’s predicted difference and today’s actual ridership. To remedy time-dependency issues, we’ll use lagged values of dif1. Using a lag of 7 is the equivalent of looking back over the last week (seven days) of ridership numbers to form future predictions. Using any other lag didn’t improve forecast accuracies. Like Model 2 other predictors like temperature and windspeed are included, except Time since `cnt is detrended. This model also includes first-differenced values of temperature; hence, we look at both a given day’s temperature and the temperature difference between that day and the previous day.\n# new dataframe that includes differenced and lagged values of cnt b2 \u0026lt;- bikes %\u0026gt;% select(cnt, weekday, mnth, holiday, temp, hum, windspeed, prec, snow, TripDate) %\u0026gt;% mutate(dif1 = c(NA, diff(cnt,1)), lag1 = lag(dif1), lag2 = lag(dif1, 2), lag3 = lag(dif1, 3), lag4 = lag(dif1, 4), lag5 = lag(dif1, 5), lag6 = lag(dif1, 6), lag7 = lag(dif1, 7), dif_temp = c(NA, diff(temp,1)) ) %\u0026gt;% filter(complete.cases(.)) # rolling window construction bike_splits2 \u0026lt;- createTimeSlices(b2$cnt, initialWindow = 2180, # row number of 12/31/2016 horizon = 30, fixedWindow = F, skip=7) library(gbm) # used for gradient boosting trees n_mods \u0026lt;- length(bike_splits2$train) rmsle_boost \u0026lt;- c() # store each RMSLE here rmse_boost\u0026lt;- c() # store each RMSE here smape_boost \u0026lt;- c() #store each SMAPE here # for each of the 42 train/test sets set.seed(100) for(i in 1:n_mods){ # this is where the original data and (differenced) predictions will be stored # will be reinitialized for each training iteration mod_df \u0026lt;- NULL rows_train \u0026lt;- bike_splits2$train[[i]] rows_test \u0026lt;- bike_splits2$test[[i]] mod_df \u0026lt;- b2[c(rows_train, rows_test), ] # dataframe containing train and test values boost.bikes=gbm(dif1 ~ .-cnt-TripDate, data=mod_df[rows_train, ], distribution=\u0026quot;gaussian\u0026quot;, n.trees=5000, interaction.depth=2) bike.boost.pred=predict(boost.bikes,newdata=mod_df[rows_test, ], n.trees=5000) mod_df \u0026lt;- mod_df %\u0026gt;% mutate(Preds = c(boost.bikes$fit, bike.boost.pred)) # store back-transformed predictions here bike_trans \u0026lt;- c() for (i in rows_test){ pred \u0026lt;- mod_df[i, \u0026quot;Preds\u0026quot;] + mod_df[i-1, \u0026quot;cnt\u0026quot;] # make any negative predictions be 0 if (pred \u0026lt; 0) {pred \u0026lt;- 0} bike_trans \u0026lt;- append(bike_trans, pred) } #calculate RMSE err_rmse \u0026lt;- Metrics::rmse(mod_df[rows_test, \u0026quot;cnt\u0026quot;], bike_trans) rmse_boost \u0026lt;- append(rmse_boost, err_rmse) # calculate RMSLE err_rmsle \u0026lt;- Metrics::rmsle(mod_df[rows_test, \u0026quot;cnt\u0026quot;], bike_trans) rmsle_boost \u0026lt;- append(rmsle_boost, err_rmsle) # calculate SMAPE err_smape \u0026lt;- Metrics::smape(mod_df[rows_test, \u0026quot;cnt\u0026quot;], bike_trans) smape_boost \u0026lt;- append(smape_boost, err_smape) } Model 2 Results This model significantly outperforms Model 1, further reducing the median RMSE from 2302 to 1642. Median accuracy is 87%. As evidenced in the mean RMSLE underpredictions are reduced, dropping from 0.269 to 0.2.\n# Median GBM RMSE paste0(\u0026quot;Median RMSE: \u0026quot;, round(median(rmse_boost),3)) [1] \u0026quot;Median RMSE: 1642.325\u0026quot; # Mean GBM RMSLE paste0(\u0026quot;Mean RMSLE: \u0026quot;, round(mean(rmsle_boost),3)) [1] \u0026quot;Mean RMSLE: 0.2\u0026quot; # Median GBM SMAPE paste0(\u0026quot;Median SMAPE: \u0026quot;, round(median(smape_boost), 2)) [1] \u0026quot;Median SMAPE: 0.13\u0026quot;   Model 3: A Naive Model – The Baseline Finally, we fit a baseline model that simply uses predicts next-day ridership to be that of the current day. Building complex models like the first two above doesn’t give us an idea of how much better they perform relative to simpler alternatives. Simple (naive) models often perform well, especially on financial data. Our naive serves a baseline to compare Model 1 and 2. If the naive performs closely to first two, the naive model should be preferred.\nThe naive model doesn’t require training and test sets, but to compare its performance with the other two models, forecasts are made only on the test set dates from the the first two models. We want all models to be compared on the same data.\n# include lag 1 of cnt variable b3 \u0026lt;- b2 %\u0026gt;% mutate(cnt_1 = lag(cnt)) %\u0026gt;% filter(complete.cases(.)) # time slices are the same as in model 2 # use the test indices rmsle_naive \u0026lt;- c() smape_naive \u0026lt;- c() rmse_naive \u0026lt;- c() for (i in 1:n_mods) { indices \u0026lt;- bike_splits2$test[[i]] actual \u0026lt;- b3[indices, \u0026quot;cnt\u0026quot;] naive_preds \u0026lt;- b3[indices, \u0026quot;cnt_1\u0026quot;] rmsle_naive \u0026lt;- append(rmsle_naive, Metrics::rmsle(actual, naive_preds)) smape_naive \u0026lt;- append(smape_naive, Metrics::smape(actual, naive_preds)) rmse_naive \u0026lt;- append(rmse_naive, Metrics::rmse(actual, naive_preds)) } # Median Naive RMSE paste0(\u0026quot;Median RMSE: \u0026quot;, round(median(rmse_naive),3)) [1] \u0026quot;Median RMSE: 2715.094\u0026quot; # Mean Naive RMSLE paste0(\u0026quot;Mean RMSLE: \u0026quot;, round(mean(rmsle_naive),3)) [1] \u0026quot;Mean RMSLE: 0.338\u0026quot; # Median Naive SMAPE paste0(\u0026quot;Median SMAPE: \u0026quot;, round(median(smape_naive), 2)) [1] \u0026quot;Median SMAPE: 0.23\u0026quot; The naive model fares rather poorly compared to the other two models on all three performance metrics.\n Comments on Model Performance Each model and its performance in the following table. Gradient boosting affords the highest accuracies on all performance metrics but is the least interpretable since each tree in gradient boosting is different, and 5000 were used in each training iteration! Volatility in daily trip numbers means relying purely on calendar and weather variables may not be sufficient for attaining higher accuracies (or lower RMSEs). For instance (as mentioned earlier) the annual National Cherry Blossom Festival is not consistent year to year, and ridership on those days isn’t easily predicted. The volatility can be seen in the numerous ridership dips in 2017, presented in the plot below. Inclusion of other variables mentioned in the Literature Review may help improve forecasts and could be used in future research.\n  Model  Median % Accuracy  Median RMSE  Mean RMSLE      Naive (Baseline)  77%  2715  0.338    MLR w/ Correlated Errors  81%  2302  0.270    Gradient Boosted Regression Trees  87%  1642  0.200     bikes %\u0026gt;% filter(between(TripDate, \u0026quot;2017/01/01\u0026quot;, \u0026quot;2017/12/31\u0026quot;)) -\u0026gt; bikes_2017 ggplot(bikes_2017, aes(TripDate,cnt)) + geom_line(col=\u0026quot;steelblue\u0026quot;) + xlab(\u0026quot;\u0026quot;)+ ylab(\u0026quot;Total Trips\u0026quot;)+ scale_x_date(breaks = \u0026quot;3 months\u0026quot;) + ggtitle(\u0026quot;Capital BikeShare Daily Trips (2017)\u0026quot;) + theme(panel.grid = element_blank(), plot.title = element_text(hjust=0.5, family = \u0026quot;Georgia\u0026quot;))  Further Research This analysis focuses on overall ridership numbers regardless of membership type (casual or registered). To predict revenue by membership type we could calculate the proportion of persons belonging to each category from historical data (say, from 2016) as an estimate. Future research can involve forecasting registered and casual ridership numbers separately, combining their forecasts for predicted overall ridership. This in itself still wouldn’t help determine revenue by membership type since memberships are broken down by five different plan types not provided in the dataset, though CB officials would theoretically have such information.\nOther research can involve station-level forecasting and use of other model techniques like neural networks.\n Conclusion We examine three competing forecasting models for 30-day ahead bike share ridership for Washington DC’s Capital BikeShare program, the best model being gradient boosted regression trees. It also improves on the best previous research by reducing the number of variables used for regression, with only 8 unique variables are used here compared to 17 by Wang et al. This work can serve as a starting point for refining forecast accuracy at the station level.\n ",
    "ref": "/project/forecasting-capital-bikeshare-ridership/"
  },{
    "title": "An Intuitive Explanation of p-values",
    "date": "",
    "description": "",
    "body": " Striving for p-value enlightenment Trying to describe p-values is notoriously difficult, even for seasoned statistics professionals. This article from FiveThirtyEight has been at the back of my head for the longest time. The author Christie Aschwanden is basically fishing around for a basic, intuitive definition of a p-value. Her conclusion?\n “What I learned by asking all these very smart people to explain p-values is that I was on a fool’s errand.”\n That’s disappointing! Any attempt to distill p-values to a pithy phrase or single sentence is bound to leave some wrong impressions, and this is a problem for educators trying to reach a general audience. However, I don’t think all hope is lost. Appealing to analogies or anecdotes often does a sufficient job. That is what I attempt to do here while minimizing the number of (statistical) spare parts left over. I’ll leave that for you to judge…\n m\u0026amp;m’s (again)   Suppose m\u0026amp;m’s claimed that their Share Size bags (3.27 oz) contain around 42 m\u0026amp;m’s. In my first blog post we supposed you went out and bought fifty such share bags and found the average number of m\u0026amp;m’s to be 29. Each package had a slightly different amount of m\u0026amp;m’s per bag, and this difference was 5.\nYou’d be justified in feeling suspicious about m\u0026amp;m’s claim of 42 since 29 seems quite far off. To investigate this, you’d ask the following question:\n How likely is it to get an average of 29 m\u0026amp;m’s if the company’s claim of 42 were true?\n Notice that if you think 29 m\u0026amp;m’s is evidence that the company is packaging less m\u0026amp;m’s than it claims, then so is 28, or 27, or 26…in fact, anything less than 29 would also count as evidence to you against m\u0026amp;m’s claim of 42. Because of this, you’d revise your question to be:\n How likely is it to get an average of 29 m\u0026amp;m’s or less, if the company’s claim of 42 were true?\n We have two competing claims: the mean number m\u0026amp;m’s is 42 and the mean number of m\u0026amp;m’s is less than 42.\nIt turns out the likelihood of seeing an average of 29 m\u0026amp;m’s or less, under the assumption the company is telling the truth, is \\(8.69*10^{-76}\\), which is a 0 followed by a decimal with 75 zeroes! That’s soooo close to 0 (the way the p-value was calculated isn’t essential to understanding p-values).\nz_score \u0026lt;- (29-42)/sqrt(25/50) pnorm(z_score) ## [1] 8.697787e-76 That right there is your p-value!\n What should we say? At this point you have two possibilities. You can say\n We were incredibly (un)lucky and just so happened to buy fifty packages of m\u0026amp;m’s that had far less than 42 m\u0026amp;m’s.  Or you can say\n This is strong evidence that m\u0026amp;m’s claim of 42 is fishy!  Given how small the p-value is, I think the second possibility makes more sense.\n Conclusion Hopefully this sheds insight on why the technical definition of a p-value is:\n the probability of observing our sample result, or results more extreme, if we assume the null hypothesis is true\n In our example, m\u0026amp;m’s claim of 42 is the null hypothesis since it describes the default claim assumed to be true. Our claim that the number of m\u0026amp;m’s is less than 42 is considered the alternative hypothesis since it challenges the default claim.\nThe “results more extreme” points back to the fact that in our m\u0026amp;m’s example, we didn’t consider 29 to be the only result to challenge the null hypothesis. Values more extreme than 29 (like 28 or 27, or 14) would also in our minds count as evidence challenging the null hypothesis of 42. Therefore, p-values must take such values into account.\n A Final Word Here’s an important point: we did not prove that m\u0026amp;m’s is lying. It entirely possible that all fifty bags we bought just so happened to have far less than 42 m\u0026amp;m’s. Maybe all those m\u0026amp;m’s packages came from a single production line that happened to be faulty. The only way to better test m\u0026amp;m’s claim is to gather more data…which means buying more m\u0026amp;m’s.\n ",
    "ref": "/post/an-intuitive-explanation-of-p-values/"
  },{
    "title": "Clearing Some Confusion about the Central Limit Theorem",
    "date": "",
    "description": "",
    "body": "  p.caption { font-size: 0.8em; }     A Normal Distribution  What’s the Problem? For quite awhile I was wondering about a potential disconnect in the way the Central Limit Theorem (a.k.a CLT) is taught and how it’s applied. Usually in statistics textbooks and online tutorials the reasoning goes like this:\n Step 1: Collect a set of observations (say, 20) and calculate the mean of that one sample of data Step 2: Collect another set of 20 observations and calculate the mean of that second sample of data Step 3: Repeat this process many, many times (say 10,000 times) Step 4: You now have 10,000 sample means. Plot a histogram of those sample means. (This is known as a distribution of sample means) Shazam! That distribution you plotted looks something like a normal distribution! That’s the CLT!  If you were to increase the set of observations collected each time from 20 to, say 25, then the resulting histogram would look even closer to a normal distribution.\nThis gives the impression it’s through repeatedly gathering sets of 20 (or 25 or more) observations we can say the distribution of the sample means is approximately normal. Boston University’s School of Public Health makes similar remarks:\n The central limit theorem states that if you…take sufficiently large random samples from the population…then the distribution of the sample means will be approximately normally distributed\n But what happens when you’re given only one sample of data? This is ordinarily the case in textbook examples and in real life, as there isn’t often enough money or time to collect more than one sample. How come we can invoke the CLT here? An example Boston University provides does just that, remarking\n probability questions about a sample mean can be addressed with the Central Limit Theorem, as long as the sample size is sufficiently large\n I bolded “sample mean” because we shifted from talking about a distribution of sample means (plural) to just a sample mean (singular)\nThis has proved confusing for a number of people, myself included, as is evidenced in this and this post from CrossValidated.\n A Sweet Diversion   Suppose you want to find the average amount of m\u0026amp;m’s in share size bags. Obviously you can’t get all share size bags on the market, so instead you buy 50 bags and meticulously count the number of m\u0026amp;m’s in each bag, which inevitably differs from bag to bag.\nFrom the thousands (if not millions) of share bags on the market (the population of m\u0026amp;m share bags), you obtained one sample of 50 bags. Suppose the mean amount of m\u0026amp;m’s in your sample turns out to be 29 and the standard deviation 5.\nThe sample mean (29) and sample variance (\\(5^{2} = 25\\)) of m\u0026amp;m’s are fantastic estimates (a.k.a unbiased estimators) of the population mean and population variance. So how can we use the sample we obtained to make inferences about the population mean of m\u0026amp;m’s in share bags? This is where the CLT comes in.\n What the CLT really says The CLT first and foremost tells us about the distribution of a sample average. We don’t need to know anything about the population itself. This is great since first, we only care about inferences on the mean, and second it can be painstakingly difficult or impossible to know everything about the entire population. Remember, we can’t buy all the m\u0026amp;m’s on the market!\nSo what can we say about the distribution of the sample average (or mean) of m\u0026amp;m’s we found? The CLT tells us:\n the shape of the distribution is approximately normal so long as the sample size \\(({n})\\) is at least 30 the mean of the distribution is equal to the mean of the population distribution the variance of the distribution is smaller than the variance of the population distribution by a factor of \\({1/n}\\)  We can demonstrate this the hard way by working through the proof of the CLT, or by using simulation - which is much more intuitive and understandable. A great walk-through of such a simulation can be found in this Khan Academy video.\nThe CLT does not require having multiple samples, as these two notable statisitics textbooks demonstrate1 2. Rather, drawing multiple samples through simulation is a way to see that the CLT makes sense.\n Back to m\u0026amp;m’s With these facts let’s return to the m\u0026amp;m’s example. Our sample size is 50, clearly greater than the requirement of 30, so we can apply the CLT. Our sample mean is 29 and our sample variance is 5. The distribution of the sample mean of share bag m\u0026amp;m’s enables us to make inferences about the population mean of share bag m\u0026amp;m’s.\nWhat does the distribution of the sample mean look like? It’ll look normally distributed with a mean of \\(29\\) and a variance of \\(25/50 = 1/2\\)\nset.seed(3000) xseq \u0026lt;-seq(26,32,.01) densities\u0026lt;-dnorm(xseq, mean=29, sd=sqrt(25/50)) plot(xseq, densities, col=\u0026quot;darkgreen\u0026quot;,xlab=\u0026quot;\u0026quot;, ylab=\u0026quot;Density\u0026quot;, type=\u0026quot;l\u0026quot;,lwd=2, cex=2, main=\u0026quot;\u0026quot;, cex.axis=.8)  Distribution of the Sample Mean number of m\u0026amp;m’s in Share Size bags  This can tell us a lot of things! For example, there’s only a 0.23% chance of finding 31 or more m\u0026amp;m’s in a share bag.\npnorm(31, mean = 29, sd = sqrt(25/50), lower.tail = F) ## [1] 0.002338867  Conclusion The main takeaway is the CLT enables us to derive the distribution of a sample mean from a just a single sample. You can use simulation to show that repeatedly obtaining samples will produce a distribution of the sample mean like the one shown for m\u0026amp;m’s above. Repeated sampling itself, though, has nothing to do with the CLT or its application.\n  The Statistical Sleuth, 3rd Edition (2013), Fred Ramsey and Daniel Schafer, pp.32-33↩\n Probabilty and Statistics for Engineering and the Sciences, 9th Edition (2016), Jay Devore, p.230↩\n   ",
    "ref": "/post/clt_revisited/"
  },{
    "title": "About",
    "date": "",
    "description": "",
    "body": " Hello and welcome to my blog! I\u0026rsquo;m Robert Hazell. My inspiration for this comes from my work I began in January 2020 as a teaching assistant to a graduate-level applied statistics course at SMU, where I\u0026rsquo;m currently completing my Masters degree in data science (expected graduation: August 2020). I received my undergrad degree from Rensselaer Polytechnic Institute.\nI started this blog in March 2020 to capture and solidify my statistics and data science musings that I hope illuminates concepts in those fields for both you and me.\nIn my spare time I love reading philosophy, biking throughout New York City, and long walks.\nI hope you find this blog useful! All feedback is welcome. Contact me via LinkedIn or by email: rmhazell@outlook.com\n",
    "ref": "/about/"
  }]
