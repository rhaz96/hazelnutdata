<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>explanatory on Statistical Musings</title>
    <link>/tags/explanatory/</link>
    <description>Recent content in explanatory on Statistical Musings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Feb 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/explanatory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Taking A Step Back On Expected Values</title>
      <link>/post/taking-a-step-back-on-expected-values/</link>
      <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/taking-a-step-back-on-expected-values/</guid>
      <description>What really is the Expected Value? I’ve been spending time reading statistician and professor Norman Matloff’s works, appreciating his skill of going “behind the scenes” on statistics concepts in Probability and Statistics for Data Science. His exposé on expected values in Chapter 3 was of particular interest. Unlike other sources that define expected value (a.k.a the mean) for a discrete random variable (say, \(X\)) using the well known formula</description>
    </item>
    
    <item>
      <title>Grokking the Permutation Test</title>
      <link>/post/grokking-the-permutation-test/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/grokking-the-permutation-test/</guid>
      <description>p.caption { font-size: 0.8em; }   Sheep wool experiment, taken from Jared Wilber  
Jared Wilber has an amazing interactive visualization illustrating the permutation test. I highly recommend reading it to get a better visual intuition of how the test statistic of the permutation test is derived.
However I feel not enough emphasis was given on the logic of why the permutation test works to begin with.</description>
    </item>
    
    <item>
      <title>An Intuitive Explanation of p-values</title>
      <link>/post/an-intuitive-explanation-of-p-values/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/an-intuitive-explanation-of-p-values/</guid>
      <description>Striving for p-value enlightenment Trying to describe p-values is notoriously difficult, even for seasoned statistics professionals. This article from FiveThirtyEight has been at the back of my head for the longest time. The author Christie Aschwanden is basically fishing around for a basic, intuitive definition of a p-value. Her conclusion?
 “What I learned by asking all these very smart people to explain p-values is that I was on a fool’s errand.</description>
    </item>
    
    <item>
      <title>Clearing Some Confusion about the Central Limit Theorem</title>
      <link>/post/clt_revisited/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/clt_revisited/</guid>
      <description>p.caption { font-size: 0.8em; }     A Normal Distribution  What’s the Problem? For quite awhile I was wondering about a potential disconnect in the way the Central Limit Theorem (a.k.a CLT) is taught and how it’s applied. Usually in statistics textbooks and online tutorials the reasoning goes like this:
 Step 1: Collect a set of observations (say, 20) and calculate the mean of that one sample of data Step 2: Collect another set of 20 observations and calculate the mean of that second sample of data Step 3: Repeat this process many, many times (say 10,000 times) Step 4: You now have 10,000 sample means.</description>
    </item>
    
  </channel>
</rss>