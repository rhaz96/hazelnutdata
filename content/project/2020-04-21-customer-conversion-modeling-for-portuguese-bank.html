---
title: Customer Conversion Modeling for Portuguese retail bank
author: ''
date: '2020-04-21'
slug: conversion-portuguese-bank
categories: []
output:
  blogdown::html_page:
    toc: true
tags:
  - python
  - projects
  - machine learning
description: ''
---


<div id="TOC">
<ul>
<li><a href="#part-i---exploratory-data-analysis">Part I - Exploratory Data Analysis</a><ul>
<li><a href="#business-understanding">Business Understanding</a></li>
<li><a href="#data-understanding">Data Understanding</a><ul>
<li><a href="#data-dictionary">Data Dictionary</a></li>
<li><a href="#verifying-data-quality">Verifying Data Quality</a></li>
<li><a href="#simple-statistics">Simple Statistics</a></li>
<li><a href="#visualize-attributes">Visualize Attributes</a></li>
<li><a href="#explore-joint-attributes">Explore Joint Attributes</a></li>
<li><a href="#explore-attributes-and-prediction-class">Explore Attributes and Prediction Class</a></li>
<li><a href="#extra-features">Extra features</a></li>
</ul></li>
<li><a href="#extras">Extras</a><ul>
<li><a href="#rejecting-the-bank-what-are-the-odds">Rejecting the bank: what are the odds?</a></li>
</ul></li>
</ul></li>
<li><a href="#part-ii---customer-conversion-modeling-with-logistic-regression">Part II - Customer Conversion Modeling with Logistic Regression</a><ul>
<li><a href="#data-cleaning-and-perform-1-hot-encoding-on-categorical-data">Data Cleaning and Perform 1 hot encoding on categorical data</a></li>
<li><a href="#training-and-testing-split">Training and Testing Split</a></li>
<li><a href="#logistic-regression-model-improvement---upsampling">Logistic Regression Model Improvement - Upsampling</a></li>
<li><a href="#test-train-split-and-upsampling">Test Train Split and Upsampling</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#interpretation-of-the-parameters-with-the-highest-weights-on-the-final-model">Interpretation of the parameters with the highest weights on the final model:</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>
</div>

<center>
Authors: Robert Hazell, <a href="https://www.linkedin.com/in/gavin-hudgeons-2b61388/">Gavin Hudgeons</a>, <a href="https://www.linkedin.com/in/bruce-lee-38227019/">Bruce Lee</a>
</center>
<pre class="python"><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
bankingDF = pd.read_csv(&quot;/Users/roberthazell/Desktop/SMU/ML1/bank-additional-full.csv&quot;)</code></pre>
<div id="part-i---exploratory-data-analysis" class="section level1">
<h1>Part I - Exploratory Data Analysis</h1>
<div id="business-understanding" class="section level2">
<h2>Business Understanding</h2>
<p>The Bank Marketing Data Set from the UCI Machine Learning Repository was selected to be analyzed by our group. The data involves information obtained from direct telemarketing campaigns from a Portuguese banking institution. The data contains 41,188 observations with 20 attributes and was collected between May 2008 to November 2010.</p>
<p>Our objective is to build a customer conversion model that predicts the likelihood a customer converts after seeing a bank term deposit offer. A cash investment, which will be held at the bank, is invested at an agreed rate of interest over a fixed period of time. Characteristics of a customer such as age, education, type of job, and many others will be analyzed to determine if they are factors determining conversion.</p>
<p>A good prediction algorithm would establish a relationship or correlation between specific attributes with the probability of whether a bank customer would subscribe to a bank term deposit. We will assess the qualities of our models using a confusion matrix. With the results from our analysis, the attributes that are strongly correlated to a customer subscribing to a bank term deposit would be emphasized in future telemarketing campaigns.</p>
</div>
<div id="data-understanding" class="section level2">
<h2>Data Understanding</h2>
<div id="data-dictionary" class="section level3">
<h3>Data Dictionary</h3>
<p>The description for the 20 data attributes are taken from: <a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing" class="uri">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>, but the complete list of features have been witheld from the dataset.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-fymr">
Attribute
</th>
<th class="tg-fymr">
Data Type
</th>
<th class="tg-fymr">
Description
</th>
<th class="tg-fymr">
Examples
</th>
</tr>
<tr>
<td class="tg-0pky">
age
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
The age of the customer in years
</td>
<td class="tg-0pky">
26, 45, 50
</td>
</tr>
<tr>
<td class="tg-0pky">
job
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
The type of job of the customer
</td>
<td class="tg-0pky">
admin, blue-collar, entrepreneur, housemaid, <br>management, retired, self-employed, services, <br>student,technician, unemployed, unknown
</td>
</tr>
<tr>
<td class="tg-0pky">
marital
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
The marital status of the customer
</td>
<td class="tg-0pky">
divorced, married, single, unknown
</td>
</tr>
<tr>
<td class="tg-0pky">
education
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
The education level of the customer
</td>
<td class="tg-0pky">
basic.4y, basic.6y, basic.9y, <br>high.school, illiterate, professional.course, <br>university.degree, unknown
</td>
</tr>
<tr>
<td class="tg-0pky">
default
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
If the customer has their credit in default
</td>
<td class="tg-0pky">
yes, no
</td>
</tr>
<tr>
<td class="tg-0pky">
housing
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
If the customer has a housing loan
</td>
<td class="tg-0pky">
yes, no
</td>
</tr>
<tr>
<td class="tg-0pky">
loan
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
If the customer has a personal loan
</td>
<td class="tg-0pky">
yes, no
</td>
</tr>
<tr>
<td class="tg-0pky">
contact
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
Customer contact communication type
</td>
<td class="tg-0pky">
cellular, telephone
</td>
</tr>
<tr>
<td class="tg-0pky">
month
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
Last contact month with the customer
</td>
<td class="tg-0pky">
jan, feb, mar, …, nov, dec
</td>
</tr>
<tr>
<td class="tg-0pky">
day_of_week
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
Last contact day of the week with the customer <br>(note that the bank is only open during weekdays)
</td>
<td class="tg-0pky">
mon, tue, wed, thu, fri
</td>
</tr>
<tr>
<td class="tg-0pky">
duration
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Duration of the last contact with customer in seconds
</td>
<td class="tg-0pky">
20, 30, 60
</td>
</tr>
<tr>
<td class="tg-0pky">
campaign
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
The number of contacts performed during <br>the campaign for the customer
</td>
<td class="tg-0pky">
3, 7, 15
</td>
</tr>
<tr>
<td class="tg-0pky">
pdays
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Number of days passed since the customer was last contacted <br>(999 means customer was never contacted)
</td>
<td class="tg-0pky">
20, 30, 999, etc.
</td>
</tr>
<tr>
<td class="tg-0pky">
previous
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Number of contacts made with customer before this campaign
</td>
<td class="tg-0pky">
20, 30, 1000, etc.
</td>
</tr>
<tr>
<td class="tg-0pky">
poutcome
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
Outcome of the previous marketing campaign
</td>
<td class="tg-0pky">
failure, nonexistent, success
</td>
</tr>
<tr>
<td class="tg-0pky">
emp.var.rate
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Employment variation rate - quarterly indicator
</td>
<td class="tg-0pky">
-1.2, 1.1, 2.3
</td>
</tr>
<tr>
<td class="tg-0pky">
cons.price.idx
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Consumer price index - monthly indicator
</td>
<td class="tg-0pky">
91.21, 93.33, 94.32
</td>
</tr>
<tr>
<td class="tg-0pky">
cons.conf.idx
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Consumer confidence index - monthly indicator
</td>
<td class="tg-0pky">
-50.8, -36.6, -26.9
</td>
</tr>
<tr>
<td class="tg-0pky">
euribor3m
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Interest rate where a selection of European banks <br>lend one another funds in euros where <br>loans have a maturity of 3 months
</td>
<td class="tg-0pky">
1.048, 4.857, 3.569
</td>
</tr>
<tr>
<td class="tg-0pky">
nr.employed
</td>
<td class="tg-0pky">
numeric
</td>
<td class="tg-0pky">
Quarterly average of the total number of employed citizens
</td>
<td class="tg-0pky">
4198, 5191, 5325
</td>
</tr>
<tr>
<td class="tg-0pky">
y
</td>
<td class="tg-0pky">
categorical
</td>
<td class="tg-0pky">
Has the client subscribed to a term deposit? (the product)
</td>
<td class="tg-0pky">
yes, no
</td>
</tr>
</table>
</div>
<div id="verifying-data-quality" class="section level3">
<h3>Verifying Data Quality</h3>
<p>The quality of the data initially appears relatively good as there are no missing or NA values. However, further reading into the documentation reveals several missing attribute values, all for categorical variables. These are encoded as “unknown”. We’ll need to decide on either treating these “unknown” values as a possible class label, delete or impute these values. The attributes that contain the “unknown” value are:</p>
<ul>
<li><code>job</code> (330 instances)</li>
<li><code>marital</code> (80 instances)</li>
<li><code>education</code> (1731 instances)</li>
<li><code>default</code> (8597 instances)</li>
<li><code>housing</code> (990 instances)</li>
<li><code>loan</code> (990 instances)</li>
</ul>
<p>For now in our exploratory data analysis we’ll keep ‘unknown’ values as a class label, but will need to re-evaluate this value in future analysis.</p>
<p>There are 12 duplicate observations, which accounts for about 0.1% of the data. However, without an attribute to indicate if each observation is unique like ‘CustomerId’, it is very possible that the duplicate observations are legitmate observations that are unrelated to the other duplicates.</p>
<p>There are outliers in several attributes. For <code>age</code> the majority of the customers fall between the ages of 17 and 60, with 10 customers over the age of 90. The <code>campaign</code> attribute, which measures how often a customer is contacted, also has some extreme values where 8 customers were contacted over 40 times during a marketing campaign. Visualization of these and other features are presented a little later. Future analysis will need to be done with and without outliers to determine any possible improvement in model adequacy.</p>
</div>
<div id="simple-statistics" class="section level3">
<h3>Simple Statistics</h3>
<p>The table below displays the simple statistics for the numeric values of the data set.</p>
<p>We see the average bank customer is around 40 years old and contacted 2 to 3 times during the marketing campaign, but rarely contacted before the marketing campaign as deduced from the means of <code>pdays</code> and <code>previous</code> attributes, which measure the number of days after the customer was last contacted from a previous campaign and the number of times the customer was contacted before the current campaign respectively. As one would expect, the targeted customers are at an age where investing makes sense. Customers in the lower age range (late teens to early 30’s) are less likely to invest their money as they may not be planning for the future or may not have the disposable income to invest.</p>
<pre class="python"><code>bankingDF.describe()</code></pre>
<pre><code>               age      duration  ...     euribor3m   nr.employed
count  41188.00000  41188.000000  ...  41188.000000  41188.000000
mean      40.02406    258.285010  ...      3.621291   5167.035911
std       10.42125    259.279249  ...      1.734447     72.251528
min       17.00000      0.000000  ...      0.634000   4963.600000
25%       32.00000    102.000000  ...      1.344000   5099.100000
50%       38.00000    180.000000  ...      4.857000   5191.000000
75%       47.00000    319.000000  ...      4.961000   5228.100000
max       98.00000   4918.000000  ...      5.045000   5228.100000

[8 rows x 10 columns]</code></pre>
<p>Strictly viewing the results of if a customer subscribed or not, the percentage of customers who did subscribe is a little over 11%.</p>
<pre class="python"><code># The percentage of individuals that subscribed to the bank
len(bankingDF[bankingDF.y==&#39;yes&#39;])/len(bankingDF)*100.0</code></pre>
<pre><code>11.265417111780131</code></pre>
<p>In the data set, over 60% of the customers are married as opposed to being single, divorced or a small percentage being unknown. This makes sense as married customers are most likely to invest for the future whether it being to purchase a home or to pay for their children’s college education.</p>
<pre class="python"><code># Marital status basic statistics
bankingDF.marital.describe()</code></pre>
<pre><code>count       41188
unique          4
top       married
freq        24928
Name: marital, dtype: object</code></pre>
</div>
<div id="visualize-attributes" class="section level3">
<h3>Visualize Attributes</h3>
<div id="age-of-potential-customers" class="section level4">
<h4>Age of Potential Customers</h4>
<pre class="python"><code># histogram of age
ax = sns.distplot(bankingDF[&quot;age&quot;])
ax.grid(False)
ax</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>There is some right skew in terms of the age range the bank targets, with the majority of customers targeted in their 30s-40s. This makes sense as those in that age bracket would more likely have money to spend on long-term deposits given a stable income. Individuals past 60 years old are not considered, most likely since such are already retired; they should have made long-term deposits decades before!</p>
</div>
<div id="job-categories" class="section level4">
<h4>Job Categories</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
job_totals = bankingDF[&quot;job&quot;].value_counts()
job_totals = pd.DataFrame({&quot;Job Type&quot;: job_totals.index,
                         &quot;Total&quot;: job_totals.values})

ax = sns.barplot(x = &quot;Total&quot;, y=&quot;Job Type&quot;, data=job_totals,
                 palette=&quot;Blues_d&quot;)
ax.grid(False)
ax</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-7-1.png" width="1440" /></p>
<p>The bank tends to heavily profile blue-collar workers in that three of the top four job types are in the blue-collar sector. The number one category – <code>admin</code> – is white-collar.</p>
</div>
<div id="how-frequent-a-prospect-is-called-current-campaign" class="section level4">
<h4>How frequent a prospect is called (current campaign)</h4>
<pre class="python"><code>plt.boxplot(bankingDF[&quot;campaign&quot;], vert=False)</code></pre>
<pre><code>{&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D object at 0x127a38710&gt;, &lt;matplotlib.lines.Line2D object at 0x127a69550&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D object at 0x127a69828&gt;, &lt;matplotlib.lines.Line2D object at 0x127a69f28&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D object at 0x127a38eb8&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D object at 0x1290361d0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D object at 0x129036860&gt;], &#39;means&#39;: []}</code></pre>
<pre class="python"><code>plt.xlabel(&quot;Number of times contacted&quot;)
plt.grid(False)
fig = plt.gcf()
a = plt.gca()
a.axes.get_yaxis().set_ticks([])</code></pre>
<pre><code>[]</code></pre>
<pre class="python"><code>fig.set_size_inches(18.5, 5)
fig</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-8-1.png" width="1776" /></p>
<p>On average the bank has been in contact less than 5 times for any given customer, but several were constantly targeted. Perhaps these are highly promising but wavering customers.</p>
</div>
<div id="when-are-calls-made" class="section level4">
<h4>When are calls made?</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
# count number of calls by day
call_day = bankingDF[&quot;day_of_week&quot;].value_counts()
# convert to dataframe
call_day = pd.DataFrame({&quot;Day&quot;: call_day.index,
                         &quot;Total&quot;: call_day.values})
# plot barplot of counts
ax = sns.barplot(call_day[&quot;Day&quot;], call_day[&quot;Total&quot;],
           palette=&quot;Purples_d&quot;)
ax.grid(False)
plt.xlabel(&#39;&#39;)
plt.ylabel(&quot;Calls Made&quot;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-9-1.png" width="1440" /></p>
<p>In terms of follow-up call days, bankers show some preference, most following up on Thursday and least on Fridays, but these differences don’t seem significant.</p>
</div>
<div id="time-elapsed-since-a-prospect-was-previously-called" class="section level4">
<h4>Time elapsed since a prospect was previously called</h4>
<pre class="python"><code># plotting how long it took for a prospect to receive
# another campaign call

# remove people who were contacted for the first time
bank_pdays_filtered = bankingDF[bankingDF[&quot;pdays&quot;] != 999]
# plot data this way to change x-axis label from default
ax = sns.violinplot(x=&quot;pdays&quot;,
                   data = bank_pdays_filtered,
                   color=&quot;turquoise&quot;)
ax.set(xlabel = &quot;Days since last contact&quot;)
ax.grid(False)
ax</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-10-1.png" width="1440" /></p>
<p>We see evidence of bimodality and right-skew from this violinplot, aspects not clearly seen in the inscribed boxplot.</p>
<p>Bankers followed up with them roughly between 3 and 6 days after their latest call for most people in this data. The two peaks are perhaps indicative of early stage sale-pitching by the banks where communication is frequent. For those in late-stage or completed negotiations, or people who constantly ignore campaign calls, the right skew probably reflects such cases.</p>
</div>
<div id="education-level-of-prospects" class="section level4">
<h4>Education level of prospects</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
education_totals = bankingDF[&quot;education&quot;].value_counts()
education_totals = pd.DataFrame({&quot;Education Type&quot;: education_totals.index,
                         &quot;Total&quot;: education_totals.values})

my_range=range(1,len(education_totals.index)+1)
plt.hlines(y=my_range, xmin=0, xmax=education_totals[&#39;Total&#39;], color=&#39;skyblue&#39;)
plt.plot(education_totals[&#39;Total&#39;], my_range, &quot;o&quot;)

plt.yticks(my_range, education_totals[&#39;Education Type&#39;])</code></pre>
<pre><code>([&lt;matplotlib.axis.YTick object at 0x1279cce48&gt;, &lt;matplotlib.axis.YTick object at 0x1279cc828&gt;, &lt;matplotlib.axis.YTick object at 0x1279c0c50&gt;, &lt;matplotlib.axis.YTick object at 0x1279ef940&gt;, &lt;matplotlib.axis.YTick object at 0x1279efdd8&gt;, &lt;matplotlib.axis.YTick object at 0x114d3e2b0&gt;, &lt;matplotlib.axis.YTick object at 0x114d3e748&gt;, &lt;matplotlib.axis.YTick object at 0x114d3ebe0&gt;], [Text(0, 0, &#39;university.degree&#39;), Text(0, 0, &#39;high.school&#39;), Text(0, 0, &#39;basic.9y&#39;), Text(0, 0, &#39;professional.course&#39;), Text(0, 0, &#39;basic.4y&#39;), Text(0, 0, &#39;basic.6y&#39;), Text(0, 0, &#39;unknown&#39;), Text(0, 0, &#39;illiterate&#39;)])</code></pre>
<pre class="python"><code>plt.xlabel(&#39;People Called by Bank&#39;)
plt.ylabel(&#39;Education&#39;)
plt.grid(False)
plt.show()</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-11-1.png" width="1440" /></p>
<p>Code for lollipop chart adapted from <a href="https://python-graph-gallery.com/182-vertical-lollipop-plot/">here</a>. As you’d expect the bank tends to target those who have, at the very least, completed high school. Greatest preference is shown towards college educated individuals. Again, this makes sense since they tend to earn more money relative to other education groups, making them likelier to deposit money.</p>
</div>
</div>
<div id="explore-joint-attributes" class="section level3">
<h3>Explore Joint Attributes</h3>
<div id="job-and-education" class="section level4">
<h4>Job and Education</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
sns.heatmap(pd.crosstab(bankingDF[&quot;education&quot;],bankingDF[&quot;job&quot;]),
            annot=True,ax=ax, fmt=&quot;g&quot;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-12-1.png" width="1440" /></p>
<p>We can visualize a cross tabulation between education level and job status. There isn’t much to be said for <code>illiterate</code> folks, but for those with <code>unknown</code> education tend to work blue-collar, admin, and technician type jobs. The most blue collar employees are those with <code>basic.4y</code>, <code>basic.6y</code>, and <code>basic.9y</code> educations. It’s not clear what these categories represent. College educated individuals work predominantly as admins, in management, or as technicians. High school grads also work in large proportion within admin, but also in the services industry.</p>
</div>
<div id="consumer-confidence-and-consumer-price-indexes" class="section level4">
<h4>Consumer Confidence and Consumer Price Indexes</h4>
<pre class="python"><code>ax = sns.lmplot( x=&quot;cons.price.idx&quot;, y=&quot;cons.conf.idx&quot;, 
               data=bankingDF, fit_reg=True, line_kws={&#39;color&#39;: &#39;red&#39;})
plt.grid(False)
plt.xlabel(&quot;Price Index&quot;)
plt.ylabel(&quot;Confidence Index&quot;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-13-1.png" width="480" /></p>
<p>Almost no correlation is present between consumer confidence and the price index. That being said, the price index is relatively consistent in this data, ranging from ~ 92 to ~ 95, so it’s unsurprising to see a flat relationship between these variables (the correlation is ~ 0.06). Consumer confidence was negative in Portugal throughout 2014 (when this data was taken) and has been that way for nearly the past decade, as can be seen <a href="https://tradingeconomics.com/portugal/consumer-confidence">here</a>. As an aside, that the correlation between price index and consumer confidence is slightly <em>positive</em> is mildly interesting.</p>
</div>
<div id="month-and-day-of-week-of-contact" class="section level4">
<h4>Month and Day of Week of Contact</h4>
<pre class="python"><code>pd.crosstab(bankingDF[&quot;day_of_week&quot;],bankingDF[&quot;month&quot;])</code></pre>
<pre><code>month        apr   aug  dec   jul   jun  mar   may  nov  oct  sep
day_of_week                                                      
fri          610  1070   24  1012  1147   94  2858  755  142  115
mon          702  1222   53  1516  1251  143  2642  766  129   90
thu          768  1347   45  1672   967   99  2537  903  163  122
tue          252  1296   25  1517   970  140  2809  814  149  118
wed          300  1243   35  1457   983   70  2923  863  135  125</code></pre>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
day_and_month = pd.crosstab(bankingDF[&quot;day_of_week&quot;],bankingDF[&quot;month&quot;])
day_and_month = day_and_month.reindex([&#39;mar&#39;, &#39;apr&#39;, &#39;may&#39;, &#39;jun&#39;, &#39;jul&#39;, &#39;aug&#39;, &#39;sep&#39;,&#39;oct&#39;, &#39;nov&#39;, &#39;dec&#39;], axis=1).reindex([&quot;mon&quot;,&quot;tue&quot;,&quot;wed&quot;,&quot;thu&quot;,&quot;fri&quot;])
ax = sns.heatmap(day_and_month, annot=True,ax=ax, fmt=&quot;d&quot;, cmap=&quot;YlGnBu&quot;)
ax.set_xlabel(&quot;&quot;)
ax.set_ylabel(&quot;&quot;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-15-1.png" width="1440" /></p>
<p>The most popular months for calling are the late spring and summer months: May through August. The least popular months were in the fall and winter (September, October, and December), although November marks an uptick in campaign calling. There is no data for January or February, implying the latest campaign began in March.</p>
</div>
<div id="timing-of-return-call-depending-on-previous-campaign-outcome" class="section level4">
<h4>Timing of return call depending on previous campaign outcome</h4>
<pre class="python"><code># filter out the first-time calls
outcome = bankingDF.loc[bankingDF[&quot;poutcome&quot;].isin([&quot;failure&quot;, &quot;success&quot;])]
# find mean time elpased from previous campaign call
outcome_tab = outcome[outcome[&quot;pdays&quot;] != 999].groupby(&quot;poutcome&quot;).agg({&quot;pdays&quot;:&quot;mean&quot;})
outcome_tab.index.names = [&quot;Prev. Call Outcome&quot;]
outcome_tab.rename(columns = {&quot;pdays&quot;: &quot;Avg. Days Until Next Call&quot;})</code></pre>
<pre><code>                    Avg. Days Until Next Call
Prev. Call Outcome                           
failure                             10.140845
success                              5.587764</code></pre>
<p>On average, if a previous campaign call failed, the bank would wait about twice as long to recall the prospect compared to recall time for previous campaign calls that succeeded.</p>
</div>
</div>
<div id="explore-attributes-and-prediction-class" class="section level3">
<h3>Explore Attributes and Prediction Class</h3>
<p>As is often the case, this class is imbalanced. People tend to reject the bank’s offers, so oversampling those who didn’t reject (those who do subscribe to a term deposit) is needed in the future.</p>
<div id="education-and-subscribers" class="section level4">
<h4>Education and Subscribers</h4>
<pre class="python"><code># all those who subscribed, grouped by education
accepted = bankingDF[bankingDF[&quot;y&quot;] == &quot;yes&quot;].groupby(&quot;education&quot;).agg({&quot;y&quot;:&quot;count&quot;}).reset_index()
accepted = accepted.sort_values(&#39;y&#39;)

# lollipop plot
my_range=range(1,len(accepted.index)+1)
sns.set_style(&#39;dark&#39;)
plt.hlines(y=my_range, xmin=0, xmax=accepted[&#39;y&#39;], color=&#39;maroon&#39;)
plt.plot(accepted[&#39;y&#39;], my_range, &quot;o&quot;, color = &#39;maroon&#39;)

plt.yticks(my_range, accepted[&#39;education&#39;])</code></pre>
<pre><code>([&lt;matplotlib.axis.YTick object at 0x12c44cb70&gt;, &lt;matplotlib.axis.YTick object at 0x135603320&gt;, &lt;matplotlib.axis.YTick object at 0x12c458b38&gt;, &lt;matplotlib.axis.YTick object at 0x12c16ce80&gt;, &lt;matplotlib.axis.YTick object at 0x12c18c358&gt;, &lt;matplotlib.axis.YTick object at 0x12c18c7f0&gt;, &lt;matplotlib.axis.YTick object at 0x12c18c588&gt;, &lt;matplotlib.axis.YTick object at 0x12c18cd30&gt;], [Text(0, 0, &#39;illiterate&#39;), Text(0, 0, &#39;basic.6y&#39;), Text(0, 0, &#39;unknown&#39;), Text(0, 0, &#39;basic.4y&#39;), Text(0, 0, &#39;basic.9y&#39;), Text(0, 0, &#39;professional.course&#39;), Text(0, 0, &#39;high.school&#39;), Text(0, 0, &#39;university.degree&#39;)])</code></pre>
<pre class="python"><code>plt.xlabel(&#39;Contacts who accepted bank offer&#39;)
plt.ylabel(&#39;Education Level&#39;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-17-1.png" width="1440" /></p>
<p>The majority who sign up with the bank are college educated, but the pattern above is almost identical for those who reject the bank. This suggests the need to examine prevalence rates across education levels, as will be seen later.</p>
</div>
<div id="call-duration-and-outcome-non-first-time-calls" class="section level4">
<h4>Call Duration and Outcome (Non First time calls)</h4>
<p>The vast majority of bank calls are to first time contacts, so we’ll like to see the call distributions for non-first timers.</p>
<pre class="python"><code>first_timers = len(bankingDF[bankingDF[&quot;pdays&quot;] == 999]) / bankingDF.shape[0]
print(&quot;Proportion of first time calls: &quot;, round(first_timers, 3))</code></pre>
<pre><code>Proportion of first time calls:  0.963</code></pre>
<pre class="python"><code>success = bankingDF[(bankingDF[&#39;y&#39;] == &quot;yes&quot;) &amp; (bankingDF[&#39;pdays&#39;]!=999)]
fail = bankingDF[(bankingDF[&#39;y&#39;] == &quot;no&quot;) &amp; (bankingDF[&#39;pdays&#39;]!=999)]

fig, ax = plt.subplots(figsize = (12,4))
for a in [success, fail]:
    sns.distplot(a[&quot;duration&quot;], bins=range(1, 3600, 20), ax=ax, kde=False)
ax.set_xlim([0, 4000])</code></pre>
<pre><code>(0.0, 4000.0)</code></pre>
<pre class="python"><code>fig.legend(labels = [&quot;Subscribed&quot;,&quot;Rejected&quot;])
plt.xlabel(&quot;Duration of Last Phone Call (Seconds)&quot;)
plt.grid(False)
plt.show()</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-19-1.png" width="1152" /></p>
<p><a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">As is pointed out</a> call duration isn’t known before the bank makes a call, and the outcome of the call is known immediately after a call. So <code>duration</code> can’t be used as a predictor when building a model. Nevertheless, for those who’ve been contacted before, bank-rejecters spend on average less time engaging with the bank agent. The distribution of talk time is right-skewed regardless of call outcome. When analyzing the response and <code>duration</code> it needs to be done in a retrospective manner.</p>
</div>
<div id="day-of-week-and-subscriptions" class="section level4">
<h4>Day of Week and Subscriptions</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(5,5)) 
sns.heatmap(pd.crosstab(bankingDF[&quot;day_of_week&quot;],bankingDF[bankingDF[&#39;y&#39;] == &quot;yes&quot;][&quot;y&quot;]).reindex([&quot;mon&quot;,&quot;tue&quot;,&quot;wed&quot;,&quot;thu&quot;,&quot;fri&quot;]),
            annot=True,ax=ax, fmt=&quot;g&quot;)

ax.set_xlabel(&quot;&quot;)
ax.set_ylabel(&quot;&quot;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-20-1.png" width="480" /></p>
<p>The most successes come on Thursdays, followed by a near tie between Tuesday and Wednesday. It seems like the beginning and end of the week are the least favorable days to call.</p>
</div>
<div id="job-profession-of-those-who-subscribe" class="section level4">
<h4>Job Profession of those who Subscribe</h4>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
# all those who subscribed, grouped by education
accepted = bankingDF[bankingDF[&quot;y&quot;] == &quot;yes&quot;].groupby(&quot;job&quot;).agg({&quot;y&quot;:&quot;count&quot;}).reset_index()
accepted = accepted.sort_values(&#39;y&#39;)

# lollipop plot
my_range=range(1,len(accepted.index)+1)
sns.set_style(&#39;dark&#39;)
plt.hlines(y=my_range, xmin=0, xmax=accepted[&#39;y&#39;])
plt.plot(accepted[&#39;y&#39;], my_range, &quot;o&quot;, color=&quot;black&quot;)

plt.yticks(my_range, accepted[&#39;job&#39;])</code></pre>
<pre><code>([&lt;matplotlib.axis.YTick object at 0x12c1db1d0&gt;, &lt;matplotlib.axis.YTick object at 0x12c1d2ac8&gt;, &lt;matplotlib.axis.YTick object at 0x130889e10&gt;, &lt;matplotlib.axis.YTick object at 0x12c2f99b0&gt;, &lt;matplotlib.axis.YTick object at 0x12c2f9e10&gt;, &lt;matplotlib.axis.YTick object at 0x12c302320&gt;, &lt;matplotlib.axis.YTick object at 0x12c302780&gt;, &lt;matplotlib.axis.YTick object at 0x12c302c18&gt;, &lt;matplotlib.axis.YTick object at 0x12c302438&gt;, &lt;matplotlib.axis.YTick object at 0x12c30b588&gt;, &lt;matplotlib.axis.YTick object at 0x12c30ba20&gt;, &lt;matplotlib.axis.YTick object at 0x12c30beb8&gt;], [Text(0, 0, &#39;unknown&#39;), Text(0, 0, &#39;housemaid&#39;), Text(0, 0, &#39;entrepreneur&#39;), Text(0, 0, &#39;unemployed&#39;), Text(0, 0, &#39;self-employed&#39;), Text(0, 0, &#39;student&#39;), Text(0, 0, &#39;services&#39;), Text(0, 0, &#39;management&#39;), Text(0, 0, &#39;retired&#39;), Text(0, 0, &#39;blue-collar&#39;), Text(0, 0, &#39;technician&#39;), Text(0, 0, &#39;admin.&#39;)])</code></pre>
<pre class="python"><code>plt.xlabel(&#39;Contacts who accepted bank offer&#39;)
plt.ylabel(&#39;Job Type&#39;)</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-21-1.png" width="1440" /></p>
<p>Interestingly, <code>retired</code> folks were one of the least targeted groups (look back at the <code>age</code> histogram) that ended up subscribing quite often.</p>
</div>
<div id="prevalance-rates-by-job-category" class="section level4">
<h4>Prevalance Rates by Job Category</h4>
<p>The plot immediately above gives the raw values of subscribers by job type. We also know the overall prevalence rate is ~ 11%. Perhaps there’s quite a bit of imbalance between education or job positions so we’ll see if prevalence rates differ by category.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,10)) 
# those who accept by job category
job_yes = (bankingDF[bankingDF[&quot;y&quot;] == &quot;yes&quot;]
    .groupby(&#39;job&#39;)
    .agg({&#39;y&#39;:&#39;count&#39;})
    .rename(columns = {&#39;y&#39;:&#39;Subscribed&#39;})
    .reset_index())

# total number of people in each job category
# regardless of subscription status
all_job = (bankingDF
    .groupby(&#39;job&#39;)
    .agg({&#39;y&#39;:&#39;count&#39;})
    .rename(columns = {&#39;y&#39;:&#39;Job Total&#39;})
    .reset_index())

# calculate prevalence rates
job_yes[&#39;Job Total&#39;] = all_job[&quot;Job Total&quot;]
job_yes[&quot;Prevalence Rate&quot;] = job_yes[&quot;Subscribed&quot;]/job_yes[&quot;Job Total&quot;]
job_yes = job_yes.sort_values(&quot;Prevalence Rate&quot;, ascending=False)


chart = sns.barplot(job_yes[&quot;job&quot;], job_yes[&quot;Prevalence Rate&quot;],
           palette=sns.cubehelix_palette(12, reverse=True))

chart.set_xticklabels(
    chart.get_xticklabels(), 
    rotation=45, 
    horizontalalignment=&#39;right&#39;,
    fontweight=&#39;light&#39;,
    fontsize=&#39;large&#39;
)</code></pre>
<pre><code>[Text(0, 0, &#39;student&#39;), Text(0, 0, &#39;retired&#39;), Text(0, 0, &#39;unemployed&#39;), Text(0, 0, &#39;admin.&#39;), Text(0, 0, &#39;management&#39;), Text(0, 0, &#39;unknown&#39;), Text(0, 0, &#39;technician&#39;), Text(0, 0, &#39;self-employed&#39;), Text(0, 0, &#39;housemaid&#39;), Text(0, 0, &#39;entrepreneur&#39;), Text(0, 0, &#39;services&#39;), Text(0, 0, &#39;blue-collar&#39;)]</code></pre>
<pre class="python"><code>chart</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-22-1.png" width="1440" /></p>
<pre class="python"><code>job_yes.set_index(&quot;job&quot;)</code></pre>
<pre><code>               Subscribed  Job Total  Prevalence Rate
job                                                  
student               275        875         0.314286
retired               434       1720         0.252326
unemployed            144       1014         0.142012
admin.               1352      10422         0.129726
management            328       2924         0.112175
unknown                37        330         0.112121
technician            730       6743         0.108260
self-employed         149       1421         0.104856
housemaid             106       1060         0.100000
entrepreneur          124       1456         0.085165
services              323       3969         0.081381
blue-collar           638       9254         0.068943</code></pre>
<p>Perhaps surprisingly, <code>student</code>s have the highest prevalence (~31.4%) albeit being one of the least targeted demographics. By contrast <code>blue-collar</code> prospects were second most called (see “Education Levels” section) yet had the lowest conversion rate at ~ 6.9%. This plot confirms raw values give an incomplete picture of conversion. We can summarize the first fact with this simple text graphic below.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(6,8)) 
plt.text(0.29, 0.6, &quot;31%&quot;, size=40,
         va=&quot;baseline&quot;, ha=&quot;right&quot;, multialignment=&quot;left&quot;,
         color = &quot;green&quot;
        )

plt.text(0.8, 0.6, &quot;of students converted...&quot;, size=16,
         va=&quot;baseline&quot;, ha=&quot;right&quot;, multialignment=&quot;left&quot;
         )

plt.text(0.15, 0.4, &quot;...yet made up only&quot;, size=16,
         va=&quot;baseline&quot;, ha=&quot;left&quot;, multialignment=&quot;left&quot;
         )

plt.text(0.57, 0.4, &quot;2%&quot;, size=20,
         va=&quot;baseline&quot;, ha=&quot;left&quot;, multialignment=&quot;left&quot;,
         color=&quot;red&quot;
         )

plt.text(0.67, 0.4, &quot;of contacts&quot;, size=16,
         va=&quot;baseline&quot;, ha=&quot;left&quot;, multialignment=&quot;left&quot;
         )

plt.gca().axes.get_yaxis().set_visible(False)
plt.gca().axes.get_xaxis().set_visible(False)
plt.show()</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-24-1.png" width="576" /></p>
<p>The bank should consider reaching out to students more.</p>
</div>
<div id="prevalence-rates-by-education-level" class="section level4">
<h4>Prevalence Rates by Education Level</h4>
<p>We now do the same analysis for education.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15,14)) 
educ_yes = (bankingDF[bankingDF[&quot;y&quot;] == &quot;yes&quot;]
    .groupby(&#39;education&#39;)
    .agg({&#39;y&#39;:&#39;count&#39;})
    .rename(columns = {&#39;y&#39;:&#39;Subscribed&#39;})
    .reset_index())

# total number of people in each job category
# regardless of subscription status
all_educ = (bankingDF
    .groupby(&#39;education&#39;)
    .agg({&#39;y&#39;:&#39;count&#39;})
    .rename(columns = {&#39;y&#39;:&#39;Education Total&#39;})
    .reset_index())

# calculate prevalence rates
educ_yes[&#39;Education Total&#39;] = all_educ[&quot;Education Total&quot;]
educ_yes[&quot;Prevalence Rate&quot;] = educ_yes[&quot;Subscribed&quot;]/educ_yes[&quot;Education Total&quot;]
educ_yes = educ_yes.sort_values(&quot;Prevalence Rate&quot;, ascending=False)


chart = sns.barplot(educ_yes[&quot;education&quot;], educ_yes[&quot;Prevalence Rate&quot;],
           color=&quot;skyblue&quot;)

chart.set_xticklabels(
    chart.get_xticklabels(), 
    rotation=45, 
    horizontalalignment=&#39;right&#39;,
    fontweight=&#39;light&#39;,
    fontsize=&#39;large&#39;
)</code></pre>
<pre><code>[Text(0, 0, &#39;illiterate&#39;), Text(0, 0, &#39;unknown&#39;), Text(0, 0, &#39;university.degree&#39;), Text(0, 0, &#39;professional.course&#39;), Text(0, 0, &#39;high.school&#39;), Text(0, 0, &#39;basic.4y&#39;), Text(0, 0, &#39;basic.6y&#39;), Text(0, 0, &#39;basic.9y&#39;)]</code></pre>
<pre class="python"><code>chart</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-25-1.png" width="1440" /></p>
<pre class="python"><code>educ_yes.set_index(&quot;education&quot;)</code></pre>
<pre><code>                     Subscribed  Education Total  Prevalence Rate
education                                                        
illiterate                    4               18         0.222222
unknown                     251             1731         0.145003
university.degree          1670            12168         0.137245
professional.course         595             5243         0.113485
high.school                1031             9515         0.108355
basic.4y                    428             4176         0.102490
basic.6y                    188             2292         0.082024
basic.9y                    473             6045         0.078246</code></pre>
<p>With only 18 <code>illiterate</code> contacts it’s no surprise seeing an inflated prevalence rate for that group. Just like job category, we see an asymmetry in education level: those with an unknown education fall nearly last in terms of being contacted (see “Education level of prospects” section) but converted second most often (or the most often if you ignore contacts who were illiterate).</p>
</div>
</div>
<div id="extra-features" class="section level3">
<h3>Extra features</h3>
<p>Here are some other variables that could widen the scope of analysis:</p>
<ul>
<li>(Monthly) Income: this adds more info than <code>loan</code> or <code>default</code></li>
<li>Current bank provider: if someone has a different bank than this Portugal bank, they may be less likely to accept</li>
<li>Does the prospect have kids?</li>
<li>If so, what is the current education level or age of the kid(s)?</li>
<li>Is the bank agent who called male or female?</li>
<li>Is the prospect male or female?</li>
</ul>
<p>A (major) caveat is some of this info may be thought too personal to share.</p>
</div>
</div>
<div id="extras" class="section level2">
<h2>Extras</h2>
<div id="rejecting-the-bank-what-are-the-odds" class="section level3">
<h3>Rejecting the bank: what are the odds?</h3>
<p>For individuals with similar age, education, job, and marital status, what are the odds someone <em>with</em> a personal loan rejecting the bank’s offer compared to that of someone <em>without</em> a personal loan rejecting the bank’s offer? We’ll look at single college graduates in the age range 30-40 who work in administrative positions. These characteristics comprise one of the largest demographics targed by the bank.</p>
<pre class="python"><code># getting the desired sample
bank_retro = bankingDF[(bankingDF[&quot;age&quot;].between(30,40)) &amp; 
          (bankingDF[&quot;education&quot;] == &quot;university.degree&quot;) &amp; 
          (bankingDF[&quot;job&quot;]==&quot;admin.&quot;) &amp;
          (bankingDF[&quot;marital&quot;] == &quot;single&quot;) &amp;
          (bankingDF[&quot;loan&quot;] != &quot;unknown&quot;)]

# two way table summary
retro_tab = pd.crosstab(bank_retro[&quot;loan&quot;], bank_retro[&quot;y&quot;], margins=True, margins_name = &quot;Total&quot;)
retro_tab.columns.names = [&quot;Subscribed?&quot;]
retro_tab.index.names = [&quot;Personal Loan?&quot;]
retro_tab</code></pre>
<pre><code>Subscribed?       no  yes  Total
Personal Loan?                  
no               976  154   1130
yes              226   23    249
Total           1202  177   1379</code></pre>
<p>From this we see, for instance, 226 people who rejected the bank (i.e. did not subscribe) had a personal loan. We can compute the odds ratio (the odds of rejecting) as follows:</p>
<ul>
<li>Odds of saying no, given that one has a personal loan: <span class="math inline">\(\frac{226}{23}\)</span></li>
<li>Odds of saying no, given that one doesn’t have a personal loan: <span class="math inline">\(\frac{976}{154}\)</span></li>
</ul>
<p>Therefore the odds of rejecting the bank’s offer if one has a personal loan is (226/23)/(976/154) = 1.55 times higher than the odds of rejecting the bank if one has no personal loan. This applies only to those with the prescribed characterisitcs above: between 30-40 years old, college grad, single, and working in admin.</p>
<p>Equivalently, the odds of someone with a personal loan rejecting the bank is 55% higher than the odds of someone without a personal loan rejecting the bank. This (again) applies only to those with the prescribed characterisitcs just mentioned.</p>
<p>If the banking agency chose its contacts based off of random sampling, then these inferences are applicable to those in the general public in the same demographic. Said differently, the scope of inference wouldn’t be restricted to just those in the current dataset. We’ll assume for simplicity that random sampling was used.</p>
<p>The upshot would then be this: the bank runs a higher risk of rejection if people in the aforementioned demographics have personal loans, which is pretty intuitive - people in debt are less likely to fork out money.</p>
</div>
</div>
</div>
<div id="part-ii---customer-conversion-modeling-with-logistic-regression" class="section level1">
<h1>Part II - Customer Conversion Modeling with Logistic Regression</h1>
<p>Part 1 dealt with exploratory analysis of the data. Now we use use logistic regression (LR) to build a customer conversion model that predicts the likelihood a customer converts after seeing a bank term deposit offer. We consider LR first since the model is highly interpretable and can serve as baseline for model performance before moving to non-parametric models (considered in Part III).</p>
<pre class="python"><code>bankingDF[[&#39;job&#39;, &#39;marital&#39;, &#39;education&#39;, &#39;default&#39;, &#39;housing&#39;, &#39;loan&#39;, &#39;contact&#39;, 
           &#39;month&#39;, &#39;day_of_week&#39;, &#39;poutcome&#39;]].describe().transpose()</code></pre>
<pre><code>             count unique                top   freq
job          41188     12             admin.  10422
marital      41188      4            married  24928
education    41188      8  university.degree  12168
default      41188      3                 no  32588
housing      41188      3                yes  21576
loan         41188      3                 no  33950
contact      41188      2           cellular  26144
month        41188     10                may  13769
day_of_week  41188      5                thu   8623
poutcome     41188      3        nonexistent  35563</code></pre>
<div id="data-cleaning-and-perform-1-hot-encoding-on-categorical-data" class="section level3">
<h3>Data Cleaning and Perform 1 hot encoding on categorical data</h3>
<p>Because the contact variable is binary, there is no need to encode it using OneHotEncoding. We can just convert it to an integer. The rest of the categorical variables will be OneHotEncoded.</p>
<p>We also get rid of the <code>duration</code> attribute since that cannot be known before the call is made. We get rid of <code>nr.employed</code> and <code>euribor3m</code> due to high correlation as mentioned earlier. We also get rid of <code>default</code> and <code>pdays</code> due to insufficient numbers of instances for different values of those classes. Then, we clean up the data by removing the columns that were OneHotEncoded, and the <code>unknown</code> values from the categorical data. The only exception will be for education, since in Part 1 we saw those with unknown education had the highest prevalence.</p>
<pre class="python"><code># we&#39;ll be using this new dataframe moving forward
bankingDF_working = bankingDF 
del bankingDF_working[&#39;duration&#39;] # removed because it is not known before the call, so it is not predictive
del bankingDF_working[&#39;nr.employed&#39;] #removed due to high correlation
del bankingDF_working[&#39;euribor3m&#39;] #removed due to high correlation
del bankingDF_working[&#39;default&#39;] # default is removed because there are only 3 instances of yes
del bankingDF_working[&#39;pdays&#39;] # 97% are uncontacted.  The rest are too spread out for meaning.  Maybe later change to boolean (wasContacted)

# function for inserting dummy variables into dataframe
# and removing the associated original column 
def banking_encoding(df, c_name):
    tmp_df = pd.get_dummies(df[c_name],prefix=c_name)
    df = pd.concat((df,tmp_df),axis=1)
    del df[c_name]
    return df
    
for c in [&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;month&#39;,&#39;day_of_week&#39;,&#39;poutcome&#39;,&#39;contact&#39;]:
    bankingDF_working = banking_encoding(bankingDF_working, c)

# Remove Unknown Categoricals from one hot encoding
del bankingDF_working[&#39;job_unknown&#39;]
del bankingDF_working[&#39;housing_unknown&#39;]
del bankingDF_working[&#39;loan_unknown&#39;]
del bankingDF_working[&#39;marital_unknown&#39;]</code></pre>
</div>
<div id="training-and-testing-split" class="section level3">
<h3>Training and Testing Split</h3>
<p>We use 80% of the instances for training and the remaining 20% for testing, utiliing 5 fold cross validation. Because of the imbalance of our response variable (y), we begin with StratifiedShuffleSplit.</p>
<pre class="python"><code>from sklearn.model_selection import StratifiedShuffleSplit
bankingDF_working.rename({&#39;y&#39;: &#39;result&#39;}, axis=&#39;columns&#39;, inplace=True)

# convert y to 1 for yes, 0 for no for precision, recall and F1 calculation
bankingDF_working[&#39;result&#39;] = bankingDF_working[&#39;result&#39;].map({&#39;yes&#39;: 1, &#39;no&#39;: 0})

y = bankingDF_working[&#39;result&#39;].values # get the labels we want
X = bankingDF_working.drop(&#39;result&#39;, axis=1).values # use everything else to predict!

num_cv_iterations = 5
#num_instances = len(y)
cv_object = StratifiedShuffleSplit(n_splits=num_cv_iterations,
                                  test_size=0.2)
                         
print(cv_object)</code></pre>
<pre><code>StratifiedShuffleSplit(n_splits=5, random_state=None, test_size=0.2,
            train_size=None)</code></pre>
<pre class="python"><code># run logistic regression and vary some parameters
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn import metrics as mt
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

iter_num=0
# the indices are the rows used for training and testing in each iteration
for train_indices, test_indices in cv_object.split(X,y): 
    
    X_train = X[train_indices]
    y_train = y[train_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]
    
    scl_obj = StandardScaler()
    scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std
    # the line of code above only looks at training data to get mean and std and we can use it 
    # to transform new feature data

    X_train_scaled = scl_obj.transform(X_train) # apply to training
    X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)

    # train the model just as before
    lr_clf = LogisticRegression(penalty=&#39;l2&#39;, C=1.0, solver=&#39;newton-cg&#39;) # get object, the &#39;C&#39; value is less (can you guess why??)
    lr_clf.fit(X_train_scaled,y_train)  # train object

    y_hat = lr_clf.predict(X_test_scaled) # get test set precitions

    # now let&#39;s get the accuracy and confusion matrix for this iterations of training/testing
    acc = mt.accuracy_score(y_test,y_hat)
    conf = mt.confusion_matrix(y_test,y_hat)
    print(&quot;====Iteration&quot;,iter_num,&quot; ====&quot;)
    print(&quot;accuracy&quot;, acc )
    print(&quot;confusion matrix\n&quot;,conf)
    
    # precision tp / (tp + fp)
    precision = precision_score(y_test, y_hat)
    print(&#39;Precision: %f&#39; % precision)
    # recall: tp / (tp + fn)
    recall = recall_score(y_test, y_hat)
    print(&#39;Recall: %f&#39; % recall)
    # f1: 2 tp / (2 tp + fp + fn)
    f1 = f1_score(y_test, y_hat)
    print(&#39;F1 score: %f\n\n&#39; % f1)
    
    iter_num+=1</code></pre>
<pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
====Iteration 0  ====
accuracy 0.9030104394270454
confusion matrix
 [[7190  120]
 [ 679  249]]
Precision: 0.674797
Recall: 0.268319
F1 score: 0.383963


StandardScaler(copy=True, with_mean=True, with_std=True)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
====Iteration 1  ====
accuracy 0.8990046127700898
confusion matrix
 [[7194  116]
 [ 716  212]]
Precision: 0.646341
Recall: 0.228448
F1 score: 0.337580


StandardScaler(copy=True, with_mean=True, with_std=True)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
====Iteration 2  ====
accuracy 0.8996115562029618
confusion matrix
 [[7210  100]
 [ 727  201]]
Precision: 0.667774
Recall: 0.216595
F1 score: 0.327095


StandardScaler(copy=True, with_mean=True, with_std=True)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
====Iteration 3  ====
accuracy 0.8986404467103666
confusion matrix
 [[7194  116]
 [ 719  209]]
Precision: 0.643077
Recall: 0.225216
F1 score: 0.333599


StandardScaler(copy=True, with_mean=True, with_std=True)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0,
                   warm_start=False)
====Iteration 4  ====
accuracy 0.8996115562029618
confusion matrix
 [[7201  109]
 [ 718  210]]
Precision: 0.658307
Recall: 0.226293
F1 score: 0.336808</code></pre>
<p>The accuracy hovers around 90% but the recall is abysmal! Again, this is a consequence of imbalanced classes. From the bank’s point of view it’s worse to have false negatives than false positives - that is, it’s worse to incorrectly predict that a customer rejects the bank. We try to remedy the model using upsampling of the minority class (<code>No</code>).</p>
</div>
<div id="logistic-regression-model-improvement---upsampling" class="section level3">
<h3>Logistic Regression Model Improvement - Upsampling</h3>
<p>Now we look at upsampling to see if imbalance issues is remedied.</p>
</div>
<div id="test-train-split-and-upsampling" class="section level3">
<h3>Test Train Split and Upsampling</h3>
<p>Here we simply use an 80/20 train-test split. In the future .this method can be extended with 5 fold CV. We make sure to upsample and scale only the training data in the process.</p>
<pre class="python"><code>from sklearn.utils import resample
from sklearn.model_selection import train_test_split

banking_train, banking_test = train_test_split(bankingDF_working, test_size=0.2)

# Separate majority and minority classes
majority = banking_train[banking_train[&#39;result&#39;]==0]
minority = banking_train[banking_train[&#39;result&#39;]==1]

minority_upsampled = resample(minority, 
                              replace=True,     # sample with replacement
                              n_samples=majority.shape[0],    # to match majority class
                              random_state=123) # reproducible results
 
# combine majority class with upsampled minority class
df_upsampled = pd.concat([majority, minority_upsampled])

# verify
print(&quot;Target variable counts:\n&quot;, df_upsampled[&#39;result&#39;].value_counts(), &quot;\n&quot;)

## ------ scale data</code></pre>
<pre><code>Target variable counts:
 1    29256
0    29256
Name: result, dtype: int64 </code></pre>
<pre class="python"><code>scl_obj = StandardScaler()

# training data
y_train = df_upsampled[&quot;result&quot;]
X_train = df_upsampled.drop(&#39;result&#39;, axis=1)

# test data
y_test = banking_test[&quot;result&quot;]
X_test = banking_test.drop(&#39;result&#39;, axis=1)

# scaling the training and test X values
scl_obj.fit(X_train)</code></pre>
<pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre>
<pre class="python"><code>X_train_scaled = scl_obj.transform(X_train)
X_test_scaled = scl_obj.transform(X_test)

## ------ fit and evaluate model
lr_clf = LogisticRegression(penalty=&#39;l2&#39;, C=1.0, class_weight=None, solver=&#39;liblinear&#39;)
lr_clf.fit(X_train_scaled,y_train)  # train object</code></pre>
<pre><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;,
                   random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0,
                   warm_start=False)</code></pre>
<pre class="python"><code>y_hat = lr_clf.predict(X_test_scaled) # get test set precitions

# print the accuracy and confusion matrix 
print(&#39;-- Results --\n&#39;)</code></pre>
<pre><code>-- Results --</code></pre>
<pre class="python"><code>print(&quot;accuracy&quot;, mt.accuracy_score(y_test,y_hat)) </code></pre>
<pre><code>accuracy 0.8199805778101481</code></pre>
<pre class="python"><code>print(&quot;confusion matrix\n&quot;,mt.confusion_matrix(y_test,y_hat))
# precision tp / (tp + fp)</code></pre>
<pre><code>confusion matrix
 [[6160 1132]
 [ 351  595]]</code></pre>
<pre class="python"><code>precision = precision_score(y_test, y_hat)
print(&#39;Precision: %f&#39; % precision)
# recall: tp / (tp + fn)</code></pre>
<pre><code>Precision: 0.344528</code></pre>
<pre class="python"><code>recall = recall_score(y_test, y_hat)
print(&#39;Recall: %f&#39; % recall)
# f1: 2 tp / (2 tp + fp + fn)</code></pre>
<pre><code>Recall: 0.628964</code></pre>
<pre class="python"><code>f1 = f1_score(y_test, y_hat)
print(&#39;F1 score: %f&#39; % f1)</code></pre>
<pre><code>F1 score: 0.445193</code></pre>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>This produces an accuracy of approximately 82%. Although this is less than the earlier models, this model would be preferred given the increase in F1 score (from 0.33 to 0.45) and dramatic increase in the recall (from ~0.2 to ~0.65). This is still overall unsatisfactory, so we’ll consider SVMs later and other algorithms in Part 3. For now we look at the regression output and a corresponding plot (which is easier to read).</p>
<pre class="python"><code>weights = pd.Series(lr_clf.coef_[0], index=df_upsampled.drop(&#39;result&#39;, axis=1).columns)
weights.plot(kind=&#39;bar&#39;, figsize=(16,20))
#plt.show()</code></pre>
<p><img src="/project/2020-04-21-customer-conversion-modeling-for-portuguese-bank_files/figure-html/unnamed-chunk-33-1.png" width="1536" /></p>
</div>
<div id="interpretation-of-the-parameters-with-the-highest-weights-on-the-final-model" class="section level3">
<h3>Interpretation of the parameters with the highest weights on the final model:</h3>
<p>Features with the highest weights were interpreted by taking the exponential of the weight to determine the odds ratio. Interpretations of the parameters with the highest weights are as follows:</p>
<p>A decrease in the Employment variation rate (emp.var.rate) changes (decreases) the odds of a customer to subscribe compared to a customer who won’t subscribe to the bank term deposit by a factor of 0.26 given all other features stay the same.</p>
<p>An increase in the Consumer price index (cons.price.idx) changes (increases) the odds of a customer to subscribe compared to one who won’t subscribe to the bank term deposit by a factor of 2.03 given all other features stay the same.</p>
<p>For customers who subscribed to a bank product previously due to a marketing campaign (poutcome_success), the odds for subscribing versus not subscribing to the bank term deposit are by a factor of 1.4 higher given all other features stay the same.</p>
<p>For customers who did not subscribe to a bank product due to a previous marketing campaign (poutcome_failure) the odds for subscribing versus not subscribing to the bank term deposit are lower by a factor of 0.8 given all other features stay the same.</p>
<p>For customers who were communicated to by cellular phone (isCellular), the odds for subscribing versus not subscribing to the bank term deposit are higher by a factor of 1.2 given that all other factors remain the same.</p>
<p>For customers who were last contacted in the month of March, the odds for subscribing compared to not subscribing to the bank term deposit are higher by a factor of 1.26, given that all other factors remain the same.</p>
<p>For customers who were last contacted in the month of May, the odds for subscribing compared to not subscribing to the bank term deposit are lower by a factor of 0.79, given that all other factors remain the same.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>We need a model that can detect higher complexity and nonlinearities in the data, seeing logistic regression has proved fruitless (even with upsampling) in achieving good predictions. In Part 3 we build and tune a random forest model. As will be seen, it performs significantly better.</p>
</div>
</div>
