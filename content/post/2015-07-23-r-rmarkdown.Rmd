---
title: "Clearing Some Confusion about the Central Limit Theorem"
author: "Robert Hazell"
date: 2020-03-24
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

For quite awhile I was wondering about a potential disconnect in the way the Central Limit Theorem (a.k.a CLT) is taught and how it's applied.  Usually in statistics textbooks and online tutorials the reasoning goes like this:

* Collect a set of observations (say, 20) and calculate the mean of that one sample of data
* Collect another set of 20 observations and calculate the mean of that second sample of data
* Repeat this process many, many times (say 10,000 times)
* You now have 10,000 sample means.  Plot a histogram of those sample means.  (This is known as a *distribution of sample means*)
* Shazam!  That distribution you plotted looks something like a normal distribution!  That's the CLT!

If you were to increase the set of observations collected each time from 20 to, say 25, then the resulting histogram would look even closer to a normal distribution.  

The scenario above gives the impression that through *repeatedly* gathering sets of 20 (or 25 or more) observations we can say the distribution of the sample means is approximately normal. Reference Boston University 

But what happens when you're given only *one* sample of data?  This is usually the case in most textbook applications.  How come we can invoke the CLT here?  I'm not the only one who's w (reference https://stats.stackexchange.com/questions/223214/why-are-all-clt-problems-using-a-single-random-sample-when-the-clt-requires-rep?noredirect=1&lq=1)










