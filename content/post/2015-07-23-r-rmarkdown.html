---
title: "Clearing Some Confusion about the Central Limit Theorem"
author: "Robert Hazell"
date: 2020-03-24
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
---



<p>For quite awhile I was wondering about a potential disconnect in the way the Central Limit Theorem (a.k.a CLT) is taught and how it’s applied. Usually in statistics textbooks and online tutorials the reasoning goes like this:</p>
<ul>
<li>Collect a set of observations (say, 20) and calculate the mean of that one sample of data</li>
<li>Collect another set of 20 observations and calculate the mean of that second sample of data</li>
<li>Repeat this process many, many times (say 10,000 times)</li>
<li>You now have 10,000 sample means. Plot a histogram of those sample means. (This is known as a <em>distribution of sample means</em>)</li>
<li>Shazam! That distribution you plotted looks something like a normal distribution! That’s the CLT!</li>
</ul>
<p>If you were to increase the set of observations collected each time from 20 to, say 25, then the resulting histogram would look even closer to a normal distribution.</p>
<p>The scenario above gives the impression that through <em>repeatedly</em> gathering sets of 20 (or 25 or more) observations we can say the distribution of the sample means is approximately normal. Reference Boston University</p>
<p>But what happens when you’re given only <em>one</em> sample of data? This is usually the case in most textbook applications. How come we can invoke the CLT here? I’m not the only one who’s had this issue (reference <a href="https://stats.stackexchange.com/questions/223214/why-are-all-clt-problems-using-a-single-random-sample-when-the-clt-requires-rep?noredirect=1&amp;lq=1" class="uri">https://stats.stackexchange.com/questions/223214/why-are-all-clt-problems-using-a-single-random-sample-when-the-clt-requires-rep?noredirect=1&amp;lq=1</a>)</p>
